{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"human_activity_train.csv\")\n",
    "test = pd.read_csv(\"human_activity_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "0         -0.923527         -0.934724  ...                        -0.710304   \n",
       "1         -0.957686         -0.943068  ...                        -0.861499   \n",
       "2         -0.977469         -0.938692  ...                        -0.760104   \n",
       "3         -0.989302         -0.938692  ...                        -0.482845   \n",
       "4         -0.990441         -0.942469  ...                        -0.699205   \n",
       "\n",
       "   angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "0                    -0.112754                              0.030400   \n",
       "1                     0.053477                             -0.007435   \n",
       "2                    -0.118559                              0.177899   \n",
       "3                    -0.036788                             -0.012892   \n",
       "4                     0.123320                              0.122542   \n",
       "\n",
       "   angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "0                         -0.464761                             -0.018446   \n",
       "1                         -0.732626                              0.703511   \n",
       "2                          0.100699                              0.808529   \n",
       "3                          0.640011                             -0.485366   \n",
       "4                          0.693578                             -0.615971   \n",
       "\n",
       "   angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  subject  \\\n",
       "0             -0.841247              0.179941             -0.058627        1   \n",
       "1             -0.844788              0.180289             -0.054317        1   \n",
       "2             -0.848933              0.180637             -0.049118        1   \n",
       "3             -0.848649              0.181935             -0.047663        1   \n",
       "4             -0.847865              0.185151             -0.043892        1   \n",
       "\n",
       "   Activity  \n",
       "0  STANDING  \n",
       "1  STANDING  \n",
       "2  STANDING  \n",
       "3  STANDING  \n",
       "4  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 563)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 563)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.drop('Activity',axis = 1).values\n",
    "train_label = train.Activity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.drop('Activity',axis = 1).values\n",
    "test_label = test.Activity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train_label)\n",
    "train_label=le.transform(train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(test_label)\n",
    "test_label=le.transform(test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 5, 5, 0, 0, 3, 0, 4, 2, 1, 1, 4, 2, 5, 5, 2, 5, 0, 5, 3, 5,\n",
       "       5, 4, 4, 2, 3, 1, 4, 2, 1, 5, 0, 2, 5, 0, 1, 1, 5, 3, 3, 1, 2, 1,\n",
       "       1, 0, 3, 2, 5, 3, 0, 0, 2, 2, 3, 4, 2, 4, 4, 0, 5, 1, 3, 4, 2, 4,\n",
       "       2, 2, 1, 1, 2, 5, 5, 3, 3, 0, 2, 3, 1, 0, 4, 5, 3, 3, 1, 0, 4, 3,\n",
       "       3, 2, 5, 1, 0, 4, 1, 0, 2, 5, 0, 3, 5, 4, 2, 5, 0, 0, 5, 0, 0, 3,\n",
       "       1, 5, 5, 0, 2, 4, 5, 5, 2, 1, 4, 1, 0, 2, 2, 5, 5, 2, 4, 4, 3, 0,\n",
       "       2, 2, 3, 5, 0, 3, 4, 1, 3, 0, 1, 3, 0, 5, 2, 2, 0, 1, 0, 3, 2, 1,\n",
       "       0, 3, 0, 3, 4, 4, 3, 5, 2, 4, 0, 3, 3, 5, 2, 1, 2, 5, 4, 5, 1, 0,\n",
       "       3, 2, 5, 4, 2, 2, 4, 5, 2, 2, 3, 0, 3, 3, 1, 0, 0, 0, 5, 1, 3, 5,\n",
       "       5, 4, 0, 0, 1, 3, 5, 3, 3, 4, 3, 2, 3, 0, 0, 0, 4, 1, 5, 4, 1, 3,\n",
       "       0, 0, 1, 3, 1, 1, 0, 1, 4, 3, 2, 2, 4, 1, 5, 0, 0, 4, 5, 0, 4, 3,\n",
       "       4, 0, 1, 0, 1, 0, 0, 2, 3, 0, 1, 2, 3, 2, 0, 5, 1, 5, 4, 2, 3, 3,\n",
       "       1, 0, 0, 5, 2, 0, 2, 0, 4, 4, 3, 4, 3, 1, 3, 4, 4, 5, 2, 3, 2, 0,\n",
       "       2, 1, 2, 2, 4, 1, 3, 5, 2, 0, 1, 3, 0, 2, 2, 1, 2, 5, 2, 1, 0, 4,\n",
       "       3, 2, 4, 0, 3, 0, 3, 5, 5, 1, 2, 2, 0, 1, 1, 1, 1, 4, 1, 4, 2, 3,\n",
       "       4, 0, 2, 4, 2, 5, 3, 4, 4, 1, 4, 3, 0, 0, 2, 2, 0, 4, 4, 1, 3, 0,\n",
       "       3, 2, 1, 2, 1, 4, 3, 0, 0, 2, 1, 2, 2, 0, 4, 0, 1, 2, 3, 4, 0, 5,\n",
       "       0, 0, 0, 5, 5, 3, 5, 4, 1, 1, 0, 0, 3, 0, 3, 4, 0, 5, 3, 0, 3, 0,\n",
       "       5, 0, 4, 1, 2, 2, 0, 3, 3, 2, 1, 2, 4, 2, 4, 1, 0, 1, 4, 0, 2, 2,\n",
       "       0, 3, 4, 2, 0, 5, 2, 5, 1, 0, 3, 5, 5, 4, 3, 1, 5, 1, 3, 5, 5, 2,\n",
       "       0, 3, 0, 2, 0, 2, 5, 4, 1, 2, 4, 0, 2, 0, 5, 4, 3, 3, 5, 5, 4, 2,\n",
       "       2, 2, 1, 1, 0, 1, 3, 5, 0, 4, 5, 5, 3, 5, 0, 5, 0, 4, 3, 0, 5, 2,\n",
       "       5, 4, 5, 4, 4, 5, 5, 4, 4, 1, 5, 5, 4, 2, 1, 2, 5, 1, 3, 1, 5, 2,\n",
       "       4, 1, 3, 1, 0, 2, 1, 2, 4, 5, 2, 5, 3, 0, 4, 1, 5, 5, 5, 0, 2, 4,\n",
       "       0, 5, 4, 1, 3, 4, 1, 4, 3, 2, 5, 1, 2, 0, 5, 0, 0, 0, 3, 2, 1, 0,\n",
       "       2, 3, 0, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1, 5, 3, 0, 1, 3, 4, 0, 2, 4,\n",
       "       3, 3, 1, 1, 0, 2, 4, 1, 4, 0, 3, 3, 2, 5, 1, 3, 0, 4, 1, 0, 2, 2,\n",
       "       4, 3, 5, 1, 2, 2, 1, 2, 3, 2, 1, 3, 5, 4, 5, 2, 0, 0, 4, 4, 2, 4,\n",
       "       1, 5, 3, 3, 2, 0, 3, 3, 3, 1, 3, 0, 3, 2, 2, 3, 2, 1, 2, 2, 1, 3,\n",
       "       1, 5, 5, 5, 2, 5, 3, 4, 4, 3, 3, 1, 2, 3, 0, 4, 5, 0, 5, 3, 4, 4,\n",
       "       3, 1, 0, 3, 1, 0, 1, 0, 2, 2, 2, 4, 2, 1, 3, 3, 1, 4, 2, 2, 2, 0,\n",
       "       1, 1, 3, 3, 3, 3, 4, 0, 0, 1, 0, 2, 1, 0, 0, 4, 2, 3, 1, 5, 1, 1,\n",
       "       4, 5, 1, 2, 0, 0, 3, 0, 1, 1, 5, 5, 3, 3, 0, 3, 5, 3, 4, 0, 2, 2,\n",
       "       1, 4, 2, 0, 1, 4, 5, 1, 5, 4, 5, 2, 3, 3, 3, 2, 0, 0, 0, 4, 0, 1,\n",
       "       1, 1, 0, 5, 1, 0, 3, 4, 4, 5, 4, 2, 1, 1, 5, 3, 4, 2, 3, 5, 3, 4,\n",
       "       2, 1, 4, 1, 5, 4, 2, 2, 0, 4, 0, 3, 0, 3, 0, 2, 2, 5, 1, 5, 2, 3,\n",
       "       3, 1, 1, 2, 5, 1, 4, 3, 0, 3, 0, 0, 3, 2, 5, 1, 4, 2, 2, 1, 0, 3,\n",
       "       2, 1, 5, 1, 5, 1, 5, 4, 0, 0, 5, 4, 3, 3, 5, 5, 2, 2, 0, 0, 1, 5,\n",
       "       3, 5, 1, 3, 2, 1, 3, 2, 1, 5, 3, 3, 0, 2, 5, 2, 3, 0, 0, 5, 2, 3,\n",
       "       0, 5, 0, 3, 5, 5, 5, 3, 5, 1, 2, 2, 2, 1, 1, 0, 1, 5, 4, 5, 3, 5,\n",
       "       1, 4, 4, 3, 1, 2, 2, 2, 5, 1, 0, 3, 1, 2, 3, 0, 0, 0, 0, 2, 1, 4,\n",
       "       3, 1, 3, 1, 3, 3, 3, 3, 3, 2, 3, 1, 2, 1, 0, 3, 1, 4, 3, 4, 0, 2,\n",
       "       1, 2, 5, 4, 0, 5, 2, 4, 2, 1, 1, 2, 3, 2, 3, 2, 3, 3, 0, 5, 3, 2,\n",
       "       0, 0, 1, 4, 1, 1, 0, 2, 0, 2, 0, 5, 1, 3, 1, 1, 0, 3, 3, 4, 1, 0,\n",
       "       4, 5, 0, 1, 0, 0, 5, 4, 1, 3, 0, 3, 1, 1, 2, 5, 3, 3, 4, 5, 3, 1,\n",
       "       5, 4, 5, 4, 0, 2, 3, 0, 3])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neural_network as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying supervised neural network using multi-layer preceptron \n",
    "import sklearn.neural_network as nn \n",
    "mlpSGD  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='sgd' , verbose=10   \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpADAM =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='adam' , verbose=10  \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpLBFGS =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='lbfgs' , verbose=10  \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.16067188\n",
      "Iteration 2, loss = 1.88288897\n",
      "Iteration 3, loss = 1.66725875\n",
      "Iteration 4, loss = 1.51382589\n",
      "Iteration 5, loss = 1.37148665\n",
      "Iteration 6, loss = 1.28039397\n",
      "Iteration 7, loss = 1.20937845\n",
      "Iteration 8, loss = 1.14568953\n",
      "Iteration 9, loss = 1.08959006\n",
      "Iteration 10, loss = 1.04129710\n",
      "Iteration 11, loss = 0.99834434\n",
      "Iteration 12, loss = 0.95951983\n",
      "Iteration 13, loss = 0.92514376\n",
      "Iteration 14, loss = 0.89391233\n",
      "Iteration 15, loss = 0.86532816\n",
      "Iteration 16, loss = 0.83803696\n",
      "Iteration 17, loss = 0.81355851\n",
      "Iteration 18, loss = 0.78999203\n",
      "Iteration 19, loss = 0.76829897\n",
      "Iteration 20, loss = 0.74869826\n",
      "Iteration 21, loss = 0.72933056\n",
      "Iteration 22, loss = 0.71089285\n",
      "Iteration 23, loss = 0.69448992\n",
      "Iteration 24, loss = 0.67723106\n",
      "Iteration 25, loss = 0.66200524\n",
      "Iteration 26, loss = 0.64770267\n",
      "Iteration 27, loss = 0.63338864\n",
      "Iteration 28, loss = 0.61955992\n",
      "Iteration 29, loss = 0.60650405\n",
      "Iteration 30, loss = 0.59424194\n",
      "Iteration 31, loss = 0.58219395\n",
      "Iteration 32, loss = 0.57107141\n",
      "Iteration 33, loss = 0.55942279\n",
      "Iteration 34, loss = 0.54901645\n",
      "Iteration 35, loss = 0.53839411\n",
      "Iteration 36, loss = 0.52884050\n",
      "Iteration 37, loss = 0.51883375\n",
      "Iteration 38, loss = 0.51005855\n",
      "Iteration 39, loss = 0.50092833\n",
      "Iteration 40, loss = 0.49311025\n",
      "Iteration 41, loss = 0.48383892\n",
      "Iteration 42, loss = 0.47674202\n",
      "Iteration 43, loss = 0.46841834\n",
      "Iteration 44, loss = 0.46106814\n",
      "Iteration 45, loss = 0.45343143\n",
      "Iteration 46, loss = 0.44656523\n",
      "Iteration 47, loss = 0.43981211\n",
      "Iteration 48, loss = 0.43276474\n",
      "Iteration 49, loss = 0.42639487\n",
      "Iteration 50, loss = 0.42027778\n",
      "Iteration 51, loss = 0.41398707\n",
      "Iteration 52, loss = 0.40817887\n",
      "Iteration 53, loss = 0.40299272\n",
      "Iteration 54, loss = 0.39684033\n",
      "Iteration 55, loss = 0.39163849\n",
      "Iteration 56, loss = 0.38627836\n",
      "Iteration 57, loss = 0.38088080\n",
      "Iteration 58, loss = 0.37579149\n",
      "Iteration 59, loss = 0.37135364\n",
      "Iteration 60, loss = 0.36630681\n",
      "Iteration 61, loss = 0.36198502\n",
      "Iteration 62, loss = 0.35694160\n",
      "Iteration 63, loss = 0.35273200\n",
      "Iteration 64, loss = 0.34867732\n",
      "Iteration 65, loss = 0.34406083\n",
      "Iteration 66, loss = 0.34015732\n",
      "Iteration 67, loss = 0.33600706\n",
      "Iteration 68, loss = 0.33217612\n",
      "Iteration 69, loss = 0.32810101\n",
      "Iteration 70, loss = 0.32438718\n",
      "Iteration 71, loss = 0.32069920\n",
      "Iteration 72, loss = 0.31754824\n",
      "Iteration 73, loss = 0.31379085\n",
      "Iteration 74, loss = 0.31017169\n",
      "Iteration 75, loss = 0.30703188\n",
      "Iteration 76, loss = 0.30359984\n",
      "Iteration 77, loss = 0.30035717\n",
      "Iteration 78, loss = 0.29738291\n",
      "Iteration 79, loss = 0.29448597\n",
      "Iteration 80, loss = 0.29087172\n",
      "Iteration 81, loss = 0.28847984\n",
      "Iteration 82, loss = 0.28527686\n",
      "Iteration 83, loss = 0.28241910\n",
      "Iteration 84, loss = 0.27929027\n",
      "Iteration 85, loss = 0.27661841\n",
      "Iteration 86, loss = 0.27421807\n",
      "Iteration 87, loss = 0.27123422\n",
      "Iteration 88, loss = 0.26856750\n",
      "Iteration 89, loss = 0.26598091\n",
      "Iteration 90, loss = 0.26367261\n",
      "Iteration 91, loss = 0.26129652\n",
      "Iteration 92, loss = 0.25872310\n",
      "Iteration 93, loss = 0.25639423\n",
      "Iteration 94, loss = 0.25398111\n",
      "Iteration 95, loss = 0.25191316\n",
      "Iteration 96, loss = 0.24950208\n",
      "Iteration 97, loss = 0.24717676\n",
      "Iteration 98, loss = 0.24516869\n",
      "Iteration 99, loss = 0.24282186\n",
      "Iteration 100, loss = 0.24115313\n",
      "Iteration 101, loss = 0.23863576\n",
      "Iteration 102, loss = 0.23701752\n",
      "Iteration 103, loss = 0.23445040\n",
      "Iteration 104, loss = 0.23258598\n",
      "Iteration 105, loss = 0.23059099\n",
      "Iteration 106, loss = 0.22876698\n",
      "Iteration 107, loss = 0.22675759\n",
      "Iteration 108, loss = 0.22485387\n",
      "Iteration 109, loss = 0.22285594\n",
      "Iteration 110, loss = 0.22136195\n",
      "Iteration 111, loss = 0.21945547\n",
      "Iteration 112, loss = 0.21758161\n",
      "Iteration 113, loss = 0.21577072\n",
      "Iteration 114, loss = 0.21420468\n",
      "Iteration 115, loss = 0.21264900\n",
      "Iteration 116, loss = 0.21088178\n",
      "Iteration 117, loss = 0.20928228\n",
      "Iteration 118, loss = 0.20772609\n",
      "Iteration 119, loss = 0.20628496\n",
      "Iteration 120, loss = 0.20455084\n",
      "Iteration 121, loss = 0.20286491\n",
      "Iteration 122, loss = 0.20133954\n",
      "Iteration 123, loss = 0.19989627\n",
      "Iteration 124, loss = 0.19866273\n",
      "Iteration 125, loss = 0.19717391\n",
      "Iteration 126, loss = 0.19544243\n",
      "Iteration 127, loss = 0.19408110\n",
      "Iteration 128, loss = 0.19271358\n",
      "Iteration 129, loss = 0.19137964\n",
      "Iteration 130, loss = 0.19007321\n",
      "Iteration 131, loss = 0.18862765\n",
      "Iteration 132, loss = 0.18710450\n",
      "Iteration 133, loss = 0.18597211\n",
      "Iteration 134, loss = 0.18440769\n",
      "Iteration 135, loss = 0.18348090\n",
      "Iteration 136, loss = 0.18199206\n",
      "Iteration 137, loss = 0.18078304\n",
      "Iteration 138, loss = 0.17980637\n",
      "Iteration 139, loss = 0.17820226\n",
      "Iteration 140, loss = 0.17726527\n",
      "Iteration 141, loss = 0.17607050\n",
      "Iteration 142, loss = 0.17491067\n",
      "Iteration 143, loss = 0.17357488\n",
      "Iteration 144, loss = 0.17245377\n",
      "Iteration 145, loss = 0.17169243\n",
      "Iteration 146, loss = 0.17036850\n",
      "Iteration 147, loss = 0.16924583\n",
      "Iteration 148, loss = 0.16823778\n",
      "Iteration 149, loss = 0.16705798\n",
      "Iteration 150, loss = 0.16608013\n",
      "Iteration 151, loss = 0.16480285\n",
      "Iteration 152, loss = 0.16373675\n",
      "Iteration 153, loss = 0.16283683\n",
      "Iteration 154, loss = 0.16187763\n",
      "Iteration 155, loss = 0.16103657\n",
      "Iteration 156, loss = 0.15974095\n",
      "Iteration 157, loss = 0.15891379\n",
      "Iteration 158, loss = 0.15774643\n",
      "Iteration 159, loss = 0.15691134\n",
      "Iteration 160, loss = 0.15588448\n",
      "Iteration 161, loss = 0.15487821\n",
      "Iteration 162, loss = 0.15406075\n",
      "Iteration 163, loss = 0.15313499\n",
      "Iteration 164, loss = 0.15239866\n",
      "Iteration 165, loss = 0.15141760\n",
      "Iteration 166, loss = 0.15056767\n",
      "Iteration 167, loss = 0.14957809\n",
      "Iteration 168, loss = 0.14865329\n",
      "Iteration 169, loss = 0.14785059\n",
      "Iteration 170, loss = 0.14705759\n",
      "Iteration 171, loss = 0.14621200\n",
      "Iteration 172, loss = 0.14590029\n",
      "Iteration 173, loss = 0.14460624\n",
      "Iteration 174, loss = 0.14386121\n",
      "Iteration 175, loss = 0.14305139\n",
      "Iteration 176, loss = 0.14223293\n",
      "Iteration 177, loss = 0.14140982\n",
      "Iteration 178, loss = 0.14091238\n",
      "Iteration 179, loss = 0.13986095\n",
      "Iteration 180, loss = 0.13942621\n",
      "Iteration 181, loss = 0.13873869\n",
      "Iteration 182, loss = 0.13752342\n",
      "Iteration 183, loss = 0.13708241\n",
      "Iteration 184, loss = 0.13623372\n",
      "Iteration 185, loss = 0.13552623\n",
      "Iteration 186, loss = 0.13504407\n",
      "Iteration 187, loss = 0.13420884\n",
      "Iteration 188, loss = 0.13374559\n",
      "Iteration 189, loss = 0.13258932\n",
      "Iteration 190, loss = 0.13199913\n",
      "Iteration 191, loss = 0.13172118\n",
      "Iteration 192, loss = 0.13075345\n",
      "Iteration 193, loss = 0.12990014\n",
      "Iteration 194, loss = 0.12928529\n",
      "Iteration 195, loss = 0.12853335\n",
      "Iteration 196, loss = 0.12803036\n",
      "Iteration 197, loss = 0.12756250\n",
      "Iteration 198, loss = 0.12676338\n",
      "Iteration 199, loss = 0.12637620\n",
      "Iteration 200, loss = 0.12548880\n",
      "Iteration 201, loss = 0.12470890\n",
      "Iteration 202, loss = 0.12417790\n",
      "Iteration 203, loss = 0.12347820\n",
      "Iteration 204, loss = 0.12284172\n",
      "Iteration 205, loss = 0.12296213\n",
      "Iteration 206, loss = 0.12178535\n",
      "Iteration 207, loss = 0.12108746\n",
      "Iteration 208, loss = 0.12056343\n",
      "Iteration 209, loss = 0.11987222\n",
      "Iteration 210, loss = 0.11945065\n",
      "Iteration 211, loss = 0.11882125\n",
      "Iteration 212, loss = 0.11854723\n",
      "Iteration 213, loss = 0.11763616\n",
      "Iteration 214, loss = 0.11730926\n",
      "Iteration 215, loss = 0.11673468\n",
      "Iteration 216, loss = 0.11610746\n",
      "Iteration 217, loss = 0.11553219\n",
      "Iteration 218, loss = 0.11522886\n",
      "Iteration 219, loss = 0.11445720\n",
      "Iteration 220, loss = 0.11386186\n",
      "Iteration 221, loss = 0.11353496\n",
      "Iteration 222, loss = 0.11282351\n",
      "Iteration 223, loss = 0.11237418\n",
      "Iteration 224, loss = 0.11189626\n",
      "Iteration 225, loss = 0.11131637\n",
      "Iteration 226, loss = 0.11088298\n",
      "Iteration 227, loss = 0.11083282\n",
      "Iteration 228, loss = 0.10981375\n",
      "Iteration 229, loss = 0.10931573\n",
      "Iteration 230, loss = 0.10884298\n",
      "Iteration 231, loss = 0.10856263\n",
      "Iteration 232, loss = 0.10802358\n",
      "Iteration 233, loss = 0.10732925\n",
      "Iteration 234, loss = 0.10703802\n",
      "Iteration 235, loss = 0.10653971\n",
      "Iteration 236, loss = 0.10635024\n",
      "Iteration 237, loss = 0.10572703\n",
      "Iteration 238, loss = 0.10509929\n",
      "Iteration 239, loss = 0.10472966\n",
      "Iteration 240, loss = 0.10440357\n",
      "Iteration 241, loss = 0.10402816\n",
      "Iteration 242, loss = 0.10333638\n",
      "Iteration 243, loss = 0.10331217\n",
      "Iteration 244, loss = 0.10229844\n",
      "Iteration 245, loss = 0.10198418\n",
      "Iteration 246, loss = 0.10181302\n",
      "Iteration 247, loss = 0.10118359\n",
      "Iteration 248, loss = 0.10080178\n",
      "Iteration 249, loss = 0.10034130\n",
      "Iteration 250, loss = 0.09984498\n",
      "Iteration 251, loss = 0.09942210\n",
      "Iteration 252, loss = 0.09906024\n",
      "Iteration 253, loss = 0.09889558\n",
      "Iteration 254, loss = 0.09840376\n",
      "Iteration 255, loss = 0.09778467\n",
      "Iteration 256, loss = 0.09751739\n",
      "Iteration 257, loss = 0.09698476\n",
      "Iteration 258, loss = 0.09693043\n",
      "Iteration 259, loss = 0.09614417\n",
      "Iteration 260, loss = 0.09590664\n",
      "Iteration 261, loss = 0.09579318\n",
      "Iteration 262, loss = 0.09517823\n",
      "Iteration 263, loss = 0.09466526\n",
      "Iteration 264, loss = 0.09456383\n",
      "Iteration 265, loss = 0.09438417\n",
      "Iteration 266, loss = 0.09363920\n",
      "Iteration 267, loss = 0.09335547\n",
      "Iteration 268, loss = 0.09274315\n",
      "Iteration 269, loss = 0.09266698\n",
      "Iteration 270, loss = 0.09214947\n",
      "Iteration 271, loss = 0.09195861\n",
      "Iteration 272, loss = 0.09135224\n",
      "Iteration 273, loss = 0.09109249\n",
      "Iteration 274, loss = 0.09077443\n",
      "Iteration 275, loss = 0.09054314\n",
      "Iteration 276, loss = 0.09000729\n",
      "Iteration 277, loss = 0.08961796\n",
      "Iteration 278, loss = 0.08923515\n",
      "Iteration 279, loss = 0.08922500\n",
      "Iteration 280, loss = 0.08864137\n",
      "Iteration 281, loss = 0.08824538\n",
      "Iteration 282, loss = 0.08799795\n",
      "Iteration 283, loss = 0.08761982\n",
      "Iteration 284, loss = 0.08719184\n",
      "Iteration 285, loss = 0.08714325\n",
      "Iteration 286, loss = 0.08653708\n",
      "Iteration 287, loss = 0.08663669\n",
      "Iteration 288, loss = 0.08599333\n",
      "Iteration 289, loss = 0.08598475\n",
      "Iteration 290, loss = 0.08571848\n",
      "Iteration 291, loss = 0.08503196\n",
      "Iteration 292, loss = 0.08465844\n",
      "Iteration 293, loss = 0.08444089\n",
      "Iteration 294, loss = 0.08412604\n",
      "Iteration 295, loss = 0.08369929\n",
      "Iteration 296, loss = 0.08345015\n",
      "Iteration 297, loss = 0.08331495\n",
      "Iteration 298, loss = 0.08283056\n",
      "Iteration 299, loss = 0.08273658\n",
      "Iteration 300, loss = 0.08230539\n",
      "Iteration 301, loss = 0.08204026\n",
      "Iteration 302, loss = 0.08163073\n",
      "Iteration 303, loss = 0.08145328\n",
      "Iteration 304, loss = 0.08122326\n",
      "Iteration 305, loss = 0.08070135\n",
      "Iteration 306, loss = 0.08046895\n",
      "Iteration 307, loss = 0.08046938\n",
      "Iteration 308, loss = 0.07999032\n",
      "Iteration 309, loss = 0.07972455\n",
      "Iteration 310, loss = 0.07954312\n",
      "Iteration 311, loss = 0.07911999\n",
      "Iteration 312, loss = 0.07886024\n",
      "Iteration 313, loss = 0.07860092\n",
      "Iteration 314, loss = 0.07830716\n",
      "Iteration 315, loss = 0.07817522\n",
      "Iteration 316, loss = 0.07757513\n",
      "Iteration 317, loss = 0.07753808\n",
      "Iteration 318, loss = 0.07725693\n",
      "Iteration 319, loss = 0.07695403\n",
      "Iteration 320, loss = 0.07663501\n",
      "Iteration 321, loss = 0.07656525\n",
      "Iteration 322, loss = 0.07619566\n",
      "Iteration 323, loss = 0.07591708\n",
      "Iteration 324, loss = 0.07551085\n",
      "Iteration 325, loss = 0.07529589\n",
      "Iteration 326, loss = 0.07493449\n",
      "Iteration 327, loss = 0.07475502\n",
      "Iteration 328, loss = 0.07451529\n",
      "Iteration 329, loss = 0.07426480\n",
      "Iteration 330, loss = 0.07407519\n",
      "Iteration 331, loss = 0.07369684\n",
      "Iteration 332, loss = 0.07355414\n",
      "Iteration 333, loss = 0.07326119\n",
      "Iteration 334, loss = 0.07295989\n",
      "Iteration 335, loss = 0.07283018\n",
      "Iteration 336, loss = 0.07247135\n",
      "Iteration 337, loss = 0.07240072\n",
      "Iteration 338, loss = 0.07226727\n",
      "Iteration 339, loss = 0.07212032\n",
      "Iteration 340, loss = 0.07166889\n",
      "Iteration 341, loss = 0.07141888\n",
      "Iteration 342, loss = 0.07108084\n",
      "Iteration 343, loss = 0.07089392\n",
      "Iteration 344, loss = 0.07063629\n",
      "Iteration 345, loss = 0.07045374\n",
      "Iteration 346, loss = 0.07023178\n",
      "Iteration 347, loss = 0.06989778\n",
      "Iteration 348, loss = 0.06969711\n",
      "Iteration 349, loss = 0.06946847\n",
      "Iteration 350, loss = 0.06930985\n",
      "Iteration 351, loss = 0.06908420\n",
      "Iteration 352, loss = 0.06881651\n",
      "Iteration 353, loss = 0.06863851\n",
      "Iteration 354, loss = 0.06840730\n",
      "Iteration 355, loss = 0.06816897\n",
      "Iteration 356, loss = 0.06798097\n",
      "Iteration 357, loss = 0.06775766\n",
      "Iteration 358, loss = 0.06749796\n",
      "Iteration 359, loss = 0.06727270\n",
      "Iteration 360, loss = 0.06711382\n",
      "Iteration 361, loss = 0.06682767\n",
      "Iteration 362, loss = 0.06680232\n",
      "Iteration 363, loss = 0.06640371\n",
      "Iteration 364, loss = 0.06637120\n",
      "Iteration 365, loss = 0.06607162\n",
      "Iteration 366, loss = 0.06592643\n",
      "Iteration 367, loss = 0.06562022\n",
      "Iteration 368, loss = 0.06548462\n",
      "Iteration 369, loss = 0.06531483\n",
      "Iteration 370, loss = 0.06494530\n",
      "Iteration 371, loss = 0.06475946\n",
      "Iteration 372, loss = 0.06459946\n",
      "Iteration 373, loss = 0.06438907\n",
      "Iteration 374, loss = 0.06411528\n",
      "Iteration 375, loss = 0.06404270\n",
      "Iteration 376, loss = 0.06377214\n",
      "Iteration 377, loss = 0.06362267\n",
      "Iteration 378, loss = 0.06374091\n",
      "Iteration 379, loss = 0.06315954\n",
      "Iteration 380, loss = 0.06309224\n",
      "Iteration 381, loss = 0.06279809\n",
      "Iteration 382, loss = 0.06266642\n",
      "Iteration 383, loss = 0.06243248\n",
      "Iteration 384, loss = 0.06220307\n",
      "Iteration 385, loss = 0.06215630\n",
      "Iteration 386, loss = 0.06183743\n",
      "Iteration 387, loss = 0.06175465\n",
      "Iteration 388, loss = 0.06165572\n",
      "Iteration 389, loss = 0.06123201\n",
      "Iteration 390, loss = 0.06121678\n",
      "Iteration 391, loss = 0.06099450\n",
      "Iteration 392, loss = 0.06067540\n",
      "Iteration 393, loss = 0.06065227\n",
      "Iteration 394, loss = 0.06046697\n",
      "Iteration 395, loss = 0.06015949\n",
      "Iteration 396, loss = 0.05994575\n",
      "Iteration 397, loss = 0.05982212\n",
      "Iteration 398, loss = 0.05964592\n",
      "Iteration 399, loss = 0.05957821\n",
      "Iteration 400, loss = 0.05926741\n",
      "Iteration 401, loss = 0.05909883\n",
      "Iteration 402, loss = 0.05906653\n",
      "Iteration 403, loss = 0.05875038\n",
      "Iteration 404, loss = 0.05889578\n",
      "Iteration 405, loss = 0.05838117\n",
      "Iteration 406, loss = 0.05828575\n",
      "Iteration 407, loss = 0.05835665\n",
      "Iteration 408, loss = 0.05799301\n",
      "Iteration 409, loss = 0.05771460\n",
      "Iteration 410, loss = 0.05755191\n",
      "Iteration 411, loss = 0.05747128\n",
      "Iteration 412, loss = 0.05721850\n",
      "Iteration 413, loss = 0.05716155\n",
      "Iteration 414, loss = 0.05695843\n",
      "Iteration 415, loss = 0.05673772\n",
      "Iteration 416, loss = 0.05654330\n",
      "Iteration 417, loss = 0.05638181\n",
      "Iteration 418, loss = 0.05623445\n",
      "Iteration 419, loss = 0.05610254\n",
      "Iteration 420, loss = 0.05602077\n",
      "Iteration 421, loss = 0.05579616\n",
      "Iteration 422, loss = 0.05566467\n",
      "Iteration 423, loss = 0.05562627\n",
      "Iteration 424, loss = 0.05533699\n",
      "Iteration 425, loss = 0.05523099\n",
      "Iteration 426, loss = 0.05501369\n",
      "Iteration 427, loss = 0.05487047\n",
      "Iteration 428, loss = 0.05471051\n",
      "Iteration 429, loss = 0.05456971\n",
      "Iteration 430, loss = 0.05443791\n",
      "Iteration 431, loss = 0.05444803\n",
      "Iteration 432, loss = 0.05417802\n",
      "Iteration 433, loss = 0.05395909\n",
      "Iteration 434, loss = 0.05385805\n",
      "Iteration 435, loss = 0.05377839\n",
      "Iteration 436, loss = 0.05359899\n",
      "Iteration 437, loss = 0.05345347\n",
      "Iteration 438, loss = 0.05325366\n",
      "Iteration 439, loss = 0.05310722\n",
      "Iteration 440, loss = 0.05292210\n",
      "Iteration 441, loss = 0.05283044\n",
      "Iteration 442, loss = 0.05269341\n",
      "Iteration 443, loss = 0.05260607\n",
      "Iteration 444, loss = 0.05248079\n",
      "Iteration 445, loss = 0.05232354\n",
      "Iteration 446, loss = 0.05208511\n",
      "Iteration 447, loss = 0.05203398\n",
      "Iteration 448, loss = 0.05180741\n",
      "Iteration 449, loss = 0.05186649\n",
      "Iteration 450, loss = 0.05156234\n",
      "Iteration 451, loss = 0.05147829\n",
      "Iteration 452, loss = 0.05140955\n",
      "Iteration 453, loss = 0.05114288\n",
      "Iteration 454, loss = 0.05102412\n",
      "Iteration 455, loss = 0.05089763\n",
      "Iteration 456, loss = 0.05106949\n",
      "Iteration 457, loss = 0.05074740\n",
      "Iteration 458, loss = 0.05053797\n",
      "Iteration 459, loss = 0.05032994\n",
      "Iteration 460, loss = 0.05020412\n",
      "Iteration 461, loss = 0.05015676\n",
      "Iteration 462, loss = 0.04991746\n",
      "Iteration 463, loss = 0.04992244\n",
      "Iteration 464, loss = 0.04982377\n",
      "Iteration 465, loss = 0.04964272\n",
      "Iteration 466, loss = 0.04971763\n",
      "Iteration 467, loss = 0.04925360\n",
      "Iteration 468, loss = 0.04934578\n",
      "Iteration 469, loss = 0.04915504\n",
      "Iteration 470, loss = 0.04902522\n",
      "Iteration 471, loss = 0.04886069\n",
      "Iteration 472, loss = 0.04867084\n",
      "Iteration 473, loss = 0.04858214\n",
      "Iteration 474, loss = 0.04846285\n",
      "Iteration 475, loss = 0.04836359\n",
      "Iteration 476, loss = 0.04825456\n",
      "Iteration 477, loss = 0.04804947\n",
      "Iteration 478, loss = 0.04797115\n",
      "Iteration 479, loss = 0.04780385\n",
      "Iteration 480, loss = 0.04771073\n",
      "Iteration 481, loss = 0.04761346\n",
      "Iteration 482, loss = 0.04760419\n",
      "Iteration 483, loss = 0.04741966\n",
      "Iteration 484, loss = 0.04729497\n",
      "Iteration 485, loss = 0.04712481\n",
      "Iteration 486, loss = 0.04710462\n",
      "Iteration 487, loss = 0.04692757\n",
      "Iteration 488, loss = 0.04683577\n",
      "Iteration 489, loss = 0.04683028\n",
      "Iteration 490, loss = 0.04661594\n",
      "Iteration 491, loss = 0.04652255\n",
      "Iteration 492, loss = 0.04635137\n",
      "Iteration 493, loss = 0.04622416\n",
      "Iteration 494, loss = 0.04628499\n",
      "Iteration 495, loss = 0.04607903\n",
      "Iteration 496, loss = 0.04611124\n",
      "Iteration 497, loss = 0.04576308\n",
      "Iteration 498, loss = 0.04573214\n",
      "Iteration 499, loss = 0.04553452\n",
      "Iteration 500, loss = 0.04564883\n",
      "Iteration 501, loss = 0.04528196\n",
      "Iteration 502, loss = 0.04527855\n",
      "Iteration 503, loss = 0.04512584\n",
      "Iteration 504, loss = 0.04517151\n",
      "Iteration 505, loss = 0.04489212\n",
      "Iteration 506, loss = 0.04475394\n",
      "Iteration 507, loss = 0.04464799\n",
      "Iteration 508, loss = 0.04458897\n",
      "Iteration 509, loss = 0.04475674\n",
      "Iteration 510, loss = 0.04437660\n",
      "Iteration 511, loss = 0.04424859\n",
      "Iteration 512, loss = 0.04414549\n",
      "Iteration 513, loss = 0.04410676\n",
      "Iteration 514, loss = 0.04392583\n",
      "Iteration 515, loss = 0.04380627\n",
      "Iteration 516, loss = 0.04379207\n",
      "Iteration 517, loss = 0.04364703\n",
      "Iteration 518, loss = 0.04353322\n",
      "Iteration 519, loss = 0.04345862\n",
      "Iteration 520, loss = 0.04353844\n",
      "Iteration 521, loss = 0.04328806\n",
      "Iteration 522, loss = 0.04318049\n",
      "Iteration 523, loss = 0.04297305\n",
      "Iteration 524, loss = 0.04294965\n",
      "Iteration 525, loss = 0.04299656\n",
      "Iteration 526, loss = 0.04270514\n",
      "Iteration 527, loss = 0.04259237\n",
      "Iteration 528, loss = 0.04251015\n",
      "Iteration 529, loss = 0.04249793\n",
      "Iteration 530, loss = 0.04229198\n",
      "Iteration 531, loss = 0.04222845\n",
      "Iteration 532, loss = 0.04211584\n",
      "Iteration 533, loss = 0.04204358\n",
      "Iteration 534, loss = 0.04200913\n",
      "Iteration 535, loss = 0.04191363\n",
      "Iteration 536, loss = 0.04177711\n",
      "Iteration 537, loss = 0.04167044\n",
      "Iteration 538, loss = 0.04153563\n",
      "Iteration 539, loss = 0.04145850\n",
      "Iteration 540, loss = 0.04130697\n",
      "Iteration 541, loss = 0.04145362\n",
      "Iteration 542, loss = 0.04110449\n",
      "Iteration 543, loss = 0.04107385\n",
      "Iteration 544, loss = 0.04098554\n",
      "Iteration 545, loss = 0.04090477\n",
      "Iteration 546, loss = 0.04081119\n",
      "Iteration 547, loss = 0.04076596\n",
      "Iteration 548, loss = 0.04058142\n",
      "Iteration 549, loss = 0.04048823\n",
      "Iteration 550, loss = 0.04041892\n",
      "Iteration 551, loss = 0.04035597\n",
      "Iteration 552, loss = 0.04024373\n",
      "Iteration 553, loss = 0.04033267\n",
      "Iteration 554, loss = 0.04010249\n",
      "Iteration 555, loss = 0.03996183\n",
      "Iteration 556, loss = 0.03994465\n",
      "Iteration 557, loss = 0.03986845\n",
      "Iteration 558, loss = 0.03982080\n",
      "Iteration 559, loss = 0.03957066\n",
      "Iteration 560, loss = 0.03950255\n",
      "Iteration 561, loss = 0.03952309\n",
      "Iteration 562, loss = 0.03938377\n",
      "Iteration 563, loss = 0.03929073\n",
      "Iteration 564, loss = 0.03919251\n",
      "Iteration 565, loss = 0.03918334\n",
      "Iteration 566, loss = 0.03906283\n",
      "Iteration 567, loss = 0.03897171\n",
      "Iteration 568, loss = 0.03899361\n",
      "Iteration 569, loss = 0.03876659\n",
      "Iteration 570, loss = 0.03878911\n",
      "Iteration 571, loss = 0.03856276\n",
      "Iteration 572, loss = 0.03850912\n",
      "Iteration 573, loss = 0.03842482\n",
      "Iteration 574, loss = 0.03841011\n",
      "Iteration 575, loss = 0.03827893\n",
      "Iteration 576, loss = 0.03824371\n",
      "Iteration 577, loss = 0.03807421\n",
      "Iteration 578, loss = 0.03800785\n",
      "Iteration 579, loss = 0.03806697\n",
      "Iteration 580, loss = 0.03794637\n",
      "Iteration 581, loss = 0.03775359\n",
      "Iteration 582, loss = 0.03768272\n",
      "Iteration 583, loss = 0.03762206\n",
      "Iteration 584, loss = 0.03754159\n",
      "Iteration 585, loss = 0.03752785\n",
      "Iteration 586, loss = 0.03743626\n",
      "Iteration 587, loss = 0.03735013\n",
      "Iteration 588, loss = 0.03726068\n",
      "Iteration 589, loss = 0.03713230\n",
      "Iteration 590, loss = 0.03707460\n",
      "Iteration 591, loss = 0.03708514\n",
      "Iteration 592, loss = 0.03704215\n",
      "Iteration 593, loss = 0.03681779\n",
      "Iteration 594, loss = 0.03672209\n",
      "Iteration 595, loss = 0.03681522\n",
      "Iteration 596, loss = 0.03673697\n",
      "Iteration 597, loss = 0.03656856\n",
      "Iteration 598, loss = 0.03647062\n",
      "Iteration 599, loss = 0.03633423\n",
      "Iteration 600, loss = 0.03636606\n",
      "Iteration 601, loss = 0.03626461\n",
      "Iteration 602, loss = 0.03619387\n",
      "Iteration 603, loss = 0.03604279\n",
      "Iteration 604, loss = 0.03597346\n",
      "Iteration 605, loss = 0.03604293\n",
      "Iteration 606, loss = 0.03582457\n",
      "Iteration 607, loss = 0.03575525\n",
      "Iteration 608, loss = 0.03573254\n",
      "Iteration 609, loss = 0.03562210\n",
      "Iteration 610, loss = 0.03557517\n",
      "Iteration 611, loss = 0.03551833\n",
      "Iteration 612, loss = 0.03539098\n",
      "Iteration 613, loss = 0.03530100\n",
      "Iteration 614, loss = 0.03527109\n",
      "Iteration 615, loss = 0.03516104\n",
      "Iteration 616, loss = 0.03511801\n",
      "Iteration 617, loss = 0.03501729\n",
      "Iteration 618, loss = 0.03494573\n",
      "Iteration 619, loss = 0.03492492\n",
      "Iteration 620, loss = 0.03491671\n",
      "Iteration 621, loss = 0.03478621\n",
      "Iteration 622, loss = 0.03484015\n",
      "Iteration 623, loss = 0.03475622\n",
      "Iteration 624, loss = 0.03458761\n",
      "Iteration 625, loss = 0.03446513\n",
      "Iteration 626, loss = 0.03441108\n",
      "Iteration 627, loss = 0.03431869\n",
      "Iteration 628, loss = 0.03457562\n",
      "Iteration 629, loss = 0.03427644\n",
      "Iteration 630, loss = 0.03412910\n",
      "Iteration 631, loss = 0.03405055\n",
      "Iteration 632, loss = 0.03396405\n",
      "Iteration 633, loss = 0.03398511\n",
      "Iteration 634, loss = 0.03390007\n",
      "Iteration 635, loss = 0.03384418\n",
      "Iteration 636, loss = 0.03377833\n",
      "Iteration 637, loss = 0.03366012\n",
      "Iteration 638, loss = 0.03361917\n",
      "Iteration 639, loss = 0.03362442\n",
      "Iteration 640, loss = 0.03348146\n",
      "Iteration 641, loss = 0.03337721\n",
      "Iteration 642, loss = 0.03332881\n",
      "Iteration 643, loss = 0.03327286\n",
      "Iteration 644, loss = 0.03321596\n",
      "Iteration 645, loss = 0.03313194\n",
      "Iteration 646, loss = 0.03315596\n",
      "Iteration 647, loss = 0.03302353\n",
      "Iteration 648, loss = 0.03305576\n",
      "Iteration 649, loss = 0.03282941\n",
      "Iteration 650, loss = 0.03281921\n",
      "Iteration 651, loss = 0.03273409\n",
      "Iteration 652, loss = 0.03272054\n",
      "Iteration 653, loss = 0.03262141\n",
      "Iteration 654, loss = 0.03261622\n",
      "Iteration 655, loss = 0.03256768\n",
      "Iteration 656, loss = 0.03241950\n",
      "Iteration 657, loss = 0.03236195\n",
      "Iteration 658, loss = 0.03236460\n",
      "Iteration 659, loss = 0.03222287\n",
      "Iteration 660, loss = 0.03222014\n",
      "Iteration 661, loss = 0.03218697\n",
      "Iteration 662, loss = 0.03221867\n",
      "Iteration 663, loss = 0.03197256\n",
      "Iteration 664, loss = 0.03191163\n",
      "Iteration 665, loss = 0.03190125\n",
      "Iteration 666, loss = 0.03185482\n",
      "Iteration 667, loss = 0.03177228\n",
      "Iteration 668, loss = 0.03167821\n",
      "Iteration 669, loss = 0.03169030\n",
      "Iteration 670, loss = 0.03156702\n",
      "Iteration 671, loss = 0.03158323\n",
      "Iteration 672, loss = 0.03145246\n",
      "Iteration 673, loss = 0.03143916\n",
      "Iteration 674, loss = 0.03147025\n",
      "Iteration 675, loss = 0.03129186\n",
      "Iteration 676, loss = 0.03123035\n",
      "Iteration 677, loss = 0.03120133\n",
      "Iteration 678, loss = 0.03112441\n",
      "Iteration 679, loss = 0.03104148\n",
      "Iteration 680, loss = 0.03100550\n",
      "Iteration 681, loss = 0.03097571\n",
      "Iteration 682, loss = 0.03087430\n",
      "Iteration 683, loss = 0.03083117\n",
      "Iteration 684, loss = 0.03077748\n",
      "Iteration 685, loss = 0.03068403\n",
      "Iteration 686, loss = 0.03065837\n",
      "Iteration 687, loss = 0.03073447\n",
      "Iteration 688, loss = 0.03055685\n",
      "Iteration 689, loss = 0.03053448\n",
      "Iteration 690, loss = 0.03043056\n",
      "Iteration 691, loss = 0.03043151\n",
      "Iteration 692, loss = 0.03033244\n",
      "Iteration 693, loss = 0.03026767\n",
      "Iteration 694, loss = 0.03023470\n",
      "Iteration 695, loss = 0.03022792\n",
      "Iteration 696, loss = 0.03005302\n",
      "Iteration 697, loss = 0.03014546\n",
      "Iteration 698, loss = 0.02996363\n",
      "Iteration 699, loss = 0.02995939\n",
      "Iteration 700, loss = 0.02986437\n",
      "Iteration 701, loss = 0.02984596\n",
      "Iteration 702, loss = 0.02980794\n",
      "Iteration 703, loss = 0.02971682\n",
      "Iteration 704, loss = 0.02969044\n",
      "Iteration 705, loss = 0.02964429\n",
      "Iteration 706, loss = 0.02961092\n",
      "Iteration 707, loss = 0.02948198\n",
      "Iteration 708, loss = 0.02944335\n",
      "Iteration 709, loss = 0.02952352\n",
      "Iteration 710, loss = 0.02935300\n",
      "Iteration 711, loss = 0.02934710\n",
      "Iteration 712, loss = 0.02921940\n",
      "Iteration 713, loss = 0.02924349\n",
      "Iteration 714, loss = 0.02916141\n",
      "Iteration 715, loss = 0.02908395\n",
      "Iteration 716, loss = 0.02910659\n",
      "Iteration 717, loss = 0.02901557\n",
      "Iteration 718, loss = 0.02910018\n",
      "Iteration 719, loss = 0.02883578\n",
      "Iteration 720, loss = 0.02891450\n",
      "Iteration 721, loss = 0.02879450\n",
      "Iteration 722, loss = 0.02870622\n",
      "Iteration 723, loss = 0.02869356\n",
      "Iteration 724, loss = 0.02861894\n",
      "Iteration 725, loss = 0.02858035\n",
      "Iteration 726, loss = 0.02857757\n",
      "Iteration 727, loss = 0.02850943\n",
      "Iteration 728, loss = 0.02847901\n",
      "Iteration 729, loss = 0.02834510\n",
      "Iteration 730, loss = 0.02828701\n",
      "Iteration 731, loss = 0.02826211\n",
      "Iteration 732, loss = 0.02818806\n",
      "Iteration 733, loss = 0.02815219\n",
      "Iteration 734, loss = 0.02808965\n",
      "Iteration 735, loss = 0.02807510\n",
      "Iteration 736, loss = 0.02802181\n",
      "Iteration 737, loss = 0.02803387\n",
      "Iteration 738, loss = 0.02793040\n",
      "Iteration 739, loss = 0.02786726\n",
      "Iteration 740, loss = 0.02785172\n",
      "Iteration 741, loss = 0.02778878\n",
      "Iteration 742, loss = 0.02770884\n",
      "Iteration 743, loss = 0.02772954\n",
      "Iteration 744, loss = 0.02761465\n",
      "Iteration 745, loss = 0.02760618\n",
      "Iteration 746, loss = 0.02753822\n",
      "Iteration 747, loss = 0.02747267\n",
      "Iteration 748, loss = 0.02747469\n",
      "Iteration 749, loss = 0.02741690\n",
      "Iteration 750, loss = 0.02734908\n",
      "Iteration 751, loss = 0.02727077\n",
      "Iteration 752, loss = 0.02723394\n",
      "Iteration 753, loss = 0.02727193\n",
      "Iteration 754, loss = 0.02716970\n",
      "Iteration 755, loss = 0.02707691\n",
      "Iteration 756, loss = 0.02710779\n",
      "Iteration 757, loss = 0.02702779\n",
      "Iteration 758, loss = 0.02703268\n",
      "Iteration 759, loss = 0.02694709\n",
      "Iteration 760, loss = 0.02693825\n",
      "Iteration 761, loss = 0.02681729\n",
      "Iteration 762, loss = 0.02680914\n",
      "Iteration 763, loss = 0.02676975\n",
      "Iteration 764, loss = 0.02673457\n",
      "Iteration 765, loss = 0.02664226\n",
      "Iteration 766, loss = 0.02658653\n",
      "Iteration 767, loss = 0.02659729\n",
      "Iteration 768, loss = 0.02667721\n",
      "Iteration 769, loss = 0.02658915\n",
      "Iteration 770, loss = 0.02646736\n",
      "Iteration 771, loss = 0.02644847\n",
      "Iteration 772, loss = 0.02639904\n",
      "Iteration 773, loss = 0.02629804\n",
      "Iteration 774, loss = 0.02624684\n",
      "Iteration 775, loss = 0.02631852\n",
      "Iteration 776, loss = 0.02614320\n",
      "Iteration 777, loss = 0.02615842\n",
      "Iteration 778, loss = 0.02613939\n",
      "Iteration 779, loss = 0.02605784\n",
      "Iteration 780, loss = 0.02599279\n",
      "Iteration 781, loss = 0.02597837\n",
      "Iteration 782, loss = 0.02595470\n",
      "Iteration 783, loss = 0.02590545\n",
      "Iteration 784, loss = 0.02580073\n",
      "Iteration 785, loss = 0.02579158\n",
      "Iteration 786, loss = 0.02573281\n",
      "Iteration 787, loss = 0.02569630\n",
      "Iteration 788, loss = 0.02570895\n",
      "Iteration 789, loss = 0.02558409\n",
      "Iteration 790, loss = 0.02567724\n",
      "Iteration 791, loss = 0.02553687\n",
      "Iteration 792, loss = 0.02548762\n",
      "Iteration 793, loss = 0.02546236\n",
      "Iteration 794, loss = 0.02540326\n",
      "Iteration 795, loss = 0.02538506\n",
      "Iteration 796, loss = 0.02530321\n",
      "Iteration 797, loss = 0.02530866\n",
      "Iteration 798, loss = 0.02521923\n",
      "Iteration 799, loss = 0.02523734\n",
      "Iteration 800, loss = 0.02520297\n",
      "Iteration 801, loss = 0.02509967\n",
      "Iteration 802, loss = 0.02526124\n",
      "Iteration 803, loss = 0.02502120\n",
      "Iteration 804, loss = 0.02496336\n",
      "Iteration 805, loss = 0.02496791\n",
      "Iteration 806, loss = 0.02488752\n",
      "Iteration 807, loss = 0.02487411\n",
      "Iteration 808, loss = 0.02483413\n",
      "Iteration 809, loss = 0.02487724\n",
      "Iteration 810, loss = 0.02474208\n",
      "Iteration 811, loss = 0.02471740\n",
      "Iteration 812, loss = 0.02467217\n",
      "Iteration 813, loss = 0.02463233\n",
      "Iteration 814, loss = 0.02456546\n",
      "Iteration 815, loss = 0.02460210\n",
      "Iteration 816, loss = 0.02448762\n",
      "Iteration 817, loss = 0.02456231\n",
      "Iteration 818, loss = 0.02440540\n",
      "Iteration 819, loss = 0.02442823\n",
      "Iteration 820, loss = 0.02435358\n",
      "Iteration 821, loss = 0.02435261\n",
      "Iteration 822, loss = 0.02427659\n",
      "Iteration 823, loss = 0.02425476\n",
      "Iteration 824, loss = 0.02419872\n",
      "Iteration 825, loss = 0.02413976\n",
      "Iteration 826, loss = 0.02413089\n",
      "Iteration 827, loss = 0.02410854\n",
      "Iteration 828, loss = 0.02410876\n",
      "Iteration 829, loss = 0.02399804\n",
      "Iteration 830, loss = 0.02396868\n",
      "Iteration 831, loss = 0.02394329\n",
      "Iteration 832, loss = 0.02390067\n",
      "Iteration 833, loss = 0.02395599\n",
      "Iteration 834, loss = 0.02388658\n",
      "Iteration 835, loss = 0.02380596\n",
      "Iteration 836, loss = 0.02374166\n",
      "Iteration 837, loss = 0.02376039\n",
      "Iteration 838, loss = 0.02366264\n",
      "Iteration 839, loss = 0.02372399\n",
      "Iteration 840, loss = 0.02361093\n",
      "Iteration 841, loss = 0.02361418\n",
      "Iteration 842, loss = 0.02367662\n",
      "Iteration 843, loss = 0.02362898\n",
      "Iteration 844, loss = 0.02344505\n",
      "Iteration 845, loss = 0.02342983\n",
      "Iteration 846, loss = 0.02343012\n",
      "Iteration 847, loss = 0.02335069\n",
      "Iteration 848, loss = 0.02341996\n",
      "Iteration 849, loss = 0.02329622\n",
      "Iteration 850, loss = 0.02327520\n",
      "Iteration 851, loss = 0.02318856\n",
      "Iteration 852, loss = 0.02316941\n",
      "Iteration 853, loss = 0.02317825\n",
      "Iteration 854, loss = 0.02311769\n",
      "Iteration 855, loss = 0.02305306\n",
      "Iteration 856, loss = 0.02302961\n",
      "Iteration 857, loss = 0.02297518\n",
      "Iteration 858, loss = 0.02297742\n",
      "Iteration 859, loss = 0.02290981\n",
      "Iteration 860, loss = 0.02288496\n",
      "Iteration 861, loss = 0.02284973\n",
      "Iteration 862, loss = 0.02281736\n",
      "Iteration 863, loss = 0.02279337\n",
      "Iteration 864, loss = 0.02274337\n",
      "Iteration 865, loss = 0.02272328\n",
      "Iteration 866, loss = 0.02268024\n",
      "Iteration 867, loss = 0.02266425\n",
      "Iteration 868, loss = 0.02261815\n",
      "Iteration 869, loss = 0.02259409\n",
      "Iteration 870, loss = 0.02267016\n",
      "Iteration 871, loss = 0.02252727\n",
      "Iteration 872, loss = 0.02247975\n",
      "Iteration 873, loss = 0.02244582\n",
      "Iteration 874, loss = 0.02242420\n",
      "Iteration 875, loss = 0.02244323\n",
      "Iteration 876, loss = 0.02243981\n",
      "Iteration 877, loss = 0.02232864\n",
      "Iteration 878, loss = 0.02226436\n",
      "Iteration 879, loss = 0.02226577\n",
      "Iteration 880, loss = 0.02226706\n",
      "Iteration 881, loss = 0.02220875\n",
      "Iteration 882, loss = 0.02220687\n",
      "Iteration 883, loss = 0.02231059\n",
      "Iteration 884, loss = 0.02208456\n",
      "Iteration 885, loss = 0.02207645\n",
      "Iteration 886, loss = 0.02205416\n",
      "Iteration 887, loss = 0.02201285\n",
      "Iteration 888, loss = 0.02207721\n",
      "Iteration 889, loss = 0.02199803\n",
      "Iteration 890, loss = 0.02188023\n",
      "Iteration 891, loss = 0.02185681\n",
      "Iteration 892, loss = 0.02181436\n",
      "Iteration 893, loss = 0.02180045\n",
      "Iteration 894, loss = 0.02174976\n",
      "Iteration 895, loss = 0.02174889\n",
      "Iteration 896, loss = 0.02170154\n",
      "Iteration 897, loss = 0.02167408\n",
      "Iteration 898, loss = 0.02162868\n",
      "Iteration 899, loss = 0.02159641\n",
      "Iteration 900, loss = 0.02156280\n",
      "Iteration 901, loss = 0.02154253\n",
      "Iteration 902, loss = 0.02156339\n",
      "Iteration 903, loss = 0.02151112\n",
      "Iteration 904, loss = 0.02156255\n",
      "Iteration 905, loss = 0.02141021\n",
      "Iteration 906, loss = 0.02138176\n",
      "Iteration 907, loss = 0.02143611\n",
      "Iteration 908, loss = 0.02142140\n",
      "Iteration 909, loss = 0.02129511\n",
      "Iteration 910, loss = 0.02125263\n",
      "Iteration 911, loss = 0.02126704\n",
      "Iteration 912, loss = 0.02124286\n",
      "Iteration 913, loss = 0.02122091\n",
      "Iteration 914, loss = 0.02114597\n",
      "Iteration 915, loss = 0.02117475\n",
      "Iteration 916, loss = 0.02107581\n",
      "Iteration 917, loss = 0.02107315\n",
      "Iteration 918, loss = 0.02109782\n",
      "Iteration 919, loss = 0.02099857\n",
      "Iteration 920, loss = 0.02099233\n",
      "Iteration 921, loss = 0.02094708\n",
      "Iteration 922, loss = 0.02090356\n",
      "Iteration 923, loss = 0.02092970\n",
      "Iteration 924, loss = 0.02086324\n",
      "Iteration 925, loss = 0.02080810\n",
      "Iteration 926, loss = 0.02084929\n",
      "Iteration 927, loss = 0.02077396\n",
      "Iteration 928, loss = 0.02074960\n",
      "Iteration 929, loss = 0.02069663\n",
      "Iteration 930, loss = 0.02070322\n",
      "Iteration 931, loss = 0.02065508\n",
      "Iteration 932, loss = 0.02061483\n",
      "Iteration 933, loss = 0.02059518\n",
      "Iteration 934, loss = 0.02054005\n",
      "Iteration 935, loss = 0.02056300\n",
      "Iteration 936, loss = 0.02051198\n",
      "Iteration 937, loss = 0.02046242\n",
      "Iteration 938, loss = 0.02043769\n",
      "Iteration 939, loss = 0.02042650\n",
      "Iteration 940, loss = 0.02039988\n",
      "Iteration 941, loss = 0.02040152\n",
      "Iteration 942, loss = 0.02033674\n",
      "Iteration 943, loss = 0.02031802\n",
      "Iteration 944, loss = 0.02026625\n",
      "Iteration 945, loss = 0.02028484\n",
      "Iteration 946, loss = 0.02020694\n",
      "Iteration 947, loss = 0.02018512\n",
      "Iteration 948, loss = 0.02017055\n",
      "Iteration 949, loss = 0.02014606\n",
      "Iteration 950, loss = 0.02012628\n",
      "Iteration 951, loss = 0.02009188\n",
      "Iteration 952, loss = 0.02009880\n",
      "Iteration 953, loss = 0.02011129\n",
      "Iteration 954, loss = 0.02001927\n",
      "Iteration 955, loss = 0.01998313\n",
      "Iteration 956, loss = 0.01999767\n",
      "Iteration 957, loss = 0.01995522\n",
      "Iteration 958, loss = 0.01990253\n",
      "Iteration 959, loss = 0.01995563\n",
      "Iteration 960, loss = 0.01982923\n",
      "Iteration 961, loss = 0.01980399\n",
      "Iteration 962, loss = 0.01976462\n",
      "Iteration 963, loss = 0.01975117\n",
      "Iteration 964, loss = 0.01972346\n",
      "Iteration 965, loss = 0.01971228\n",
      "Iteration 966, loss = 0.01967184\n",
      "Iteration 967, loss = 0.01965391\n",
      "Iteration 968, loss = 0.01962246\n",
      "Iteration 969, loss = 0.01960539\n",
      "Iteration 970, loss = 0.01957388\n",
      "Iteration 971, loss = 0.01954421\n",
      "Iteration 972, loss = 0.01949764\n",
      "Iteration 973, loss = 0.01954617\n",
      "Iteration 974, loss = 0.01945722\n",
      "Iteration 975, loss = 0.01943641\n",
      "Iteration 976, loss = 0.01940942\n",
      "Iteration 977, loss = 0.01938224\n",
      "Iteration 978, loss = 0.01934563\n",
      "Iteration 979, loss = 0.01931659\n",
      "Iteration 980, loss = 0.01929554\n",
      "Iteration 981, loss = 0.01929230\n",
      "Iteration 982, loss = 0.01924864\n",
      "Iteration 983, loss = 0.01921785\n",
      "Iteration 984, loss = 0.01928843\n",
      "Iteration 985, loss = 0.01915924\n",
      "Iteration 986, loss = 0.01917139\n",
      "Iteration 987, loss = 0.01914481\n",
      "Iteration 988, loss = 0.01912145\n",
      "Iteration 989, loss = 0.01907765\n",
      "Iteration 990, loss = 0.01905925\n",
      "Iteration 991, loss = 0.01908372\n",
      "Iteration 992, loss = 0.01902148\n",
      "Iteration 993, loss = 0.01897222\n",
      "Iteration 994, loss = 0.01894648\n",
      "Iteration 995, loss = 0.01892331\n",
      "Iteration 996, loss = 0.01888965\n",
      "Iteration 997, loss = 0.01894921\n",
      "Iteration 998, loss = 0.01883033\n",
      "Iteration 999, loss = 0.01883550\n",
      "Iteration 1000, loss = 0.01881968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "nnModelSGD  = mlpSGD.fit(train_data , train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnModelBFGS = mlpLBFGS.fit(train_data , train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.67583081\n",
      "Iteration 2, loss = 1.10707632\n",
      "Iteration 3, loss = 0.83472498\n",
      "Iteration 4, loss = 0.66361538\n",
      "Iteration 5, loss = 0.55039260\n",
      "Iteration 6, loss = 0.46953234\n",
      "Iteration 7, loss = 0.41009281\n",
      "Iteration 8, loss = 0.36235774\n",
      "Iteration 9, loss = 0.32583275\n",
      "Iteration 10, loss = 0.28569054\n",
      "Iteration 11, loss = 0.26160146\n",
      "Iteration 12, loss = 0.23385899\n",
      "Iteration 13, loss = 0.21564893\n",
      "Iteration 14, loss = 0.19577075\n",
      "Iteration 15, loss = 0.18125987\n",
      "Iteration 16, loss = 0.16907194\n",
      "Iteration 17, loss = 0.15535872\n",
      "Iteration 18, loss = 0.14617789\n",
      "Iteration 19, loss = 0.13704806\n",
      "Iteration 20, loss = 0.13115148\n",
      "Iteration 21, loss = 0.12177596\n",
      "Iteration 22, loss = 0.11448169\n",
      "Iteration 23, loss = 0.10685501\n",
      "Iteration 24, loss = 0.10127356\n",
      "Iteration 25, loss = 0.09657746\n",
      "Iteration 26, loss = 0.09332302\n",
      "Iteration 27, loss = 0.09211501\n",
      "Iteration 28, loss = 0.08192979\n",
      "Iteration 29, loss = 0.08044003\n",
      "Iteration 30, loss = 0.07408339\n",
      "Iteration 31, loss = 0.07043508\n",
      "Iteration 32, loss = 0.06890111\n",
      "Iteration 33, loss = 0.06585548\n",
      "Iteration 34, loss = 0.06321461\n",
      "Iteration 35, loss = 0.06058912\n",
      "Iteration 36, loss = 0.06213909\n",
      "Iteration 37, loss = 0.06267400\n",
      "Iteration 38, loss = 0.05962496\n",
      "Iteration 39, loss = 0.05501709\n",
      "Iteration 40, loss = 0.04925974\n",
      "Iteration 41, loss = 0.04589601\n",
      "Iteration 42, loss = 0.04411572\n",
      "Iteration 43, loss = 0.04263197\n",
      "Iteration 44, loss = 0.04237776\n",
      "Iteration 45, loss = 0.03931708\n",
      "Iteration 46, loss = 0.03742951\n",
      "Iteration 47, loss = 0.03888378\n",
      "Iteration 48, loss = 0.03778046\n",
      "Iteration 49, loss = 0.03555980\n",
      "Iteration 50, loss = 0.03446070\n",
      "Iteration 51, loss = 0.03221627\n",
      "Iteration 52, loss = 0.03116217\n",
      "Iteration 53, loss = 0.03012032\n",
      "Iteration 54, loss = 0.02926994\n",
      "Iteration 55, loss = 0.02851751\n",
      "Iteration 56, loss = 0.02774555\n",
      "Iteration 57, loss = 0.02671699\n",
      "Iteration 58, loss = 0.02580669\n",
      "Iteration 59, loss = 0.02526138\n",
      "Iteration 60, loss = 0.02492880\n",
      "Iteration 61, loss = 0.02388765\n",
      "Iteration 62, loss = 0.02344633\n",
      "Iteration 63, loss = 0.02275145\n",
      "Iteration 64, loss = 0.02248116\n",
      "Iteration 65, loss = 0.02179176\n",
      "Iteration 66, loss = 0.02094417\n",
      "Iteration 67, loss = 0.02036033\n",
      "Iteration 68, loss = 0.01986550\n",
      "Iteration 69, loss = 0.01928671\n",
      "Iteration 70, loss = 0.01898745\n",
      "Iteration 71, loss = 0.01848224\n",
      "Iteration 72, loss = 0.01807481\n",
      "Iteration 73, loss = 0.01811163\n",
      "Iteration 74, loss = 0.01767658\n",
      "Iteration 75, loss = 0.01707032\n",
      "Iteration 76, loss = 0.01635341\n",
      "Iteration 77, loss = 0.01600263\n",
      "Iteration 78, loss = 0.01560734\n",
      "Iteration 79, loss = 0.01522359\n",
      "Iteration 80, loss = 0.01504917\n",
      "Iteration 81, loss = 0.01513703\n",
      "Iteration 82, loss = 0.01448778\n",
      "Iteration 83, loss = 0.01406609\n",
      "Iteration 84, loss = 0.01366785\n",
      "Iteration 85, loss = 0.01342011\n",
      "Iteration 86, loss = 0.01306304\n",
      "Iteration 87, loss = 0.01287246\n",
      "Iteration 88, loss = 0.01264009\n",
      "Iteration 89, loss = 0.01250589\n",
      "Iteration 90, loss = 0.01210205\n",
      "Iteration 91, loss = 0.01207556\n",
      "Iteration 92, loss = 0.01166049\n",
      "Iteration 93, loss = 0.01140650\n",
      "Iteration 94, loss = 0.01117541\n",
      "Iteration 95, loss = 0.01096927\n",
      "Iteration 96, loss = 0.01084429\n",
      "Iteration 97, loss = 0.01089920\n",
      "Iteration 98, loss = 0.01042308\n",
      "Iteration 99, loss = 0.01019064\n",
      "Iteration 100, loss = 0.01009269\n",
      "Iteration 101, loss = 0.00997251\n",
      "Iteration 102, loss = 0.00968629\n",
      "Iteration 103, loss = 0.00936168\n",
      "Iteration 104, loss = 0.00917992\n",
      "Iteration 105, loss = 0.00904427\n",
      "Iteration 106, loss = 0.00901408\n",
      "Iteration 107, loss = 0.00890043\n",
      "Iteration 108, loss = 0.00867150\n",
      "Iteration 109, loss = 0.00842562\n",
      "Iteration 110, loss = 0.00838187\n",
      "Iteration 111, loss = 0.00849235\n",
      "Iteration 112, loss = 0.00820408\n",
      "Iteration 113, loss = 0.00787770\n",
      "Iteration 114, loss = 0.00775327\n",
      "Iteration 115, loss = 0.00779985\n",
      "Iteration 116, loss = 0.00745828\n",
      "Iteration 117, loss = 0.00744269\n",
      "Iteration 118, loss = 0.00735675\n",
      "Iteration 119, loss = 0.00727442\n",
      "Iteration 120, loss = 0.00704570\n",
      "Iteration 121, loss = 0.00700644\n",
      "Iteration 122, loss = 0.00690485\n",
      "Iteration 123, loss = 0.00677641\n",
      "Iteration 124, loss = 0.00691178\n",
      "Iteration 125, loss = 0.00654574\n",
      "Iteration 126, loss = 0.00641387\n",
      "Iteration 127, loss = 0.00644069\n",
      "Iteration 128, loss = 0.00632298\n",
      "Iteration 129, loss = 0.00630799\n",
      "Iteration 130, loss = 0.00605057\n",
      "Iteration 131, loss = 0.00595664\n",
      "Iteration 132, loss = 0.00584034\n",
      "Iteration 133, loss = 0.00577372\n",
      "Iteration 134, loss = 0.00570450\n",
      "Iteration 135, loss = 0.00563908\n",
      "Iteration 136, loss = 0.00556453\n",
      "Iteration 137, loss = 0.00557204\n",
      "Iteration 138, loss = 0.00538902\n",
      "Iteration 139, loss = 0.00541169\n",
      "Iteration 140, loss = 0.00528222\n",
      "Iteration 141, loss = 0.00516565\n",
      "Iteration 142, loss = 0.00510091\n",
      "Iteration 143, loss = 0.00498932\n",
      "Iteration 144, loss = 0.00497344\n",
      "Iteration 145, loss = 0.00493391\n",
      "Iteration 146, loss = 0.00490336\n",
      "Iteration 147, loss = 0.00486276\n",
      "Iteration 148, loss = 0.00466647\n",
      "Iteration 149, loss = 0.00465431\n",
      "Iteration 150, loss = 0.00460439\n",
      "Iteration 151, loss = 0.00455509\n",
      "Iteration 152, loss = 0.00448828\n",
      "Iteration 153, loss = 0.00451692\n",
      "Iteration 154, loss = 0.00437755\n",
      "Iteration 155, loss = 0.00434047\n",
      "Iteration 156, loss = 0.00436107\n",
      "Iteration 157, loss = 0.00423146\n",
      "Iteration 158, loss = 0.00417277\n",
      "Iteration 159, loss = 0.00412219\n",
      "Iteration 160, loss = 0.00402362\n",
      "Iteration 161, loss = 0.00400331\n",
      "Iteration 162, loss = 0.00390670\n",
      "Iteration 163, loss = 0.00389513\n",
      "Iteration 164, loss = 0.00386325\n",
      "Iteration 165, loss = 0.00384172\n",
      "Iteration 166, loss = 0.00374310\n",
      "Iteration 167, loss = 0.00371031\n",
      "Iteration 168, loss = 0.00364678\n",
      "Iteration 169, loss = 0.00364811\n",
      "Iteration 170, loss = 0.00361731\n",
      "Iteration 171, loss = 0.00359822\n",
      "Iteration 172, loss = 0.00358941\n",
      "Iteration 173, loss = 0.00344394\n",
      "Iteration 174, loss = 0.00345050\n",
      "Iteration 175, loss = 0.00346465\n",
      "Iteration 176, loss = 0.00341634\n",
      "Iteration 177, loss = 0.00331392\n",
      "Iteration 178, loss = 0.00329960\n",
      "Iteration 179, loss = 0.00326722\n",
      "Iteration 180, loss = 0.00323350\n",
      "Iteration 181, loss = 0.00322834\n",
      "Iteration 182, loss = 0.00315082\n",
      "Iteration 183, loss = 0.00309559\n",
      "Iteration 184, loss = 0.00311560\n",
      "Iteration 185, loss = 0.00303125\n",
      "Iteration 186, loss = 0.00304536\n",
      "Iteration 187, loss = 0.00298378\n",
      "Iteration 188, loss = 0.00298054\n",
      "Iteration 189, loss = 0.00292314\n",
      "Iteration 190, loss = 0.00286745\n",
      "Iteration 191, loss = 0.00291627\n",
      "Iteration 192, loss = 0.00293504\n",
      "Iteration 193, loss = 0.00277076\n",
      "Iteration 194, loss = 0.00284966\n",
      "Iteration 195, loss = 0.00279942\n",
      "Iteration 196, loss = 0.00281996\n",
      "Iteration 197, loss = 0.00273293\n",
      "Iteration 198, loss = 0.00266572\n",
      "Iteration 199, loss = 0.00265983\n",
      "Iteration 200, loss = 0.00259927\n",
      "Iteration 201, loss = 0.00259015\n",
      "Iteration 202, loss = 0.00253678\n",
      "Iteration 203, loss = 0.00254193\n",
      "Iteration 204, loss = 0.00247021\n",
      "Iteration 205, loss = 0.00252415\n",
      "Iteration 206, loss = 0.00252664\n",
      "Iteration 207, loss = 0.00241629\n",
      "Iteration 208, loss = 0.00240137\n",
      "Iteration 209, loss = 0.00239562\n",
      "Iteration 210, loss = 0.00234030\n",
      "Iteration 211, loss = 0.00233132\n",
      "Iteration 212, loss = 0.00230865\n",
      "Iteration 213, loss = 0.00227815\n",
      "Iteration 214, loss = 0.00228707\n",
      "Iteration 215, loss = 0.00233284\n",
      "Iteration 216, loss = 0.00225605\n",
      "Iteration 217, loss = 0.00220993\n",
      "Iteration 218, loss = 0.00222572\n",
      "Iteration 219, loss = 0.00215590\n",
      "Iteration 220, loss = 0.00215159\n",
      "Iteration 221, loss = 0.00214793\n",
      "Iteration 222, loss = 0.00212425\n",
      "Iteration 223, loss = 0.00211161\n",
      "Iteration 224, loss = 0.00207526\n",
      "Iteration 225, loss = 0.00206382\n",
      "Iteration 226, loss = 0.00203225\n",
      "Iteration 227, loss = 0.00201266\n",
      "Iteration 228, loss = 0.00202637\n",
      "Iteration 229, loss = 0.00201152\n",
      "Iteration 230, loss = 0.00196426\n",
      "Iteration 231, loss = 0.00195580\n",
      "Iteration 232, loss = 0.00194824\n",
      "Iteration 233, loss = 0.00189422\n",
      "Iteration 234, loss = 0.00192235\n",
      "Iteration 235, loss = 0.00185834\n",
      "Iteration 236, loss = 0.00187427\n",
      "Iteration 237, loss = 0.00185160\n",
      "Iteration 238, loss = 0.00184399\n",
      "Iteration 239, loss = 0.00183574\n",
      "Iteration 240, loss = 0.00183893\n",
      "Iteration 241, loss = 0.00180682\n",
      "Iteration 242, loss = 0.00178441\n",
      "Iteration 243, loss = 0.00175335\n",
      "Iteration 244, loss = 0.00174080\n",
      "Iteration 245, loss = 0.00172277\n",
      "Iteration 246, loss = 0.00174091\n",
      "Iteration 247, loss = 0.00169242\n",
      "Iteration 248, loss = 0.00170172\n",
      "Iteration 249, loss = 0.00168299\n",
      "Iteration 250, loss = 0.00165993\n",
      "Iteration 251, loss = 0.00167137\n",
      "Iteration 252, loss = 0.00167681\n",
      "Iteration 253, loss = 0.00160104\n",
      "Iteration 254, loss = 0.00165003\n",
      "Iteration 255, loss = 0.00158497\n",
      "Iteration 256, loss = 0.00160666\n",
      "Iteration 257, loss = 0.00157655\n",
      "Iteration 258, loss = 0.00158654\n",
      "Iteration 259, loss = 0.00153579\n",
      "Iteration 260, loss = 0.00153730\n",
      "Iteration 261, loss = 0.00152269\n",
      "Iteration 262, loss = 0.00151131\n",
      "Iteration 263, loss = 0.00149209\n",
      "Iteration 264, loss = 0.00150951\n",
      "Iteration 265, loss = 0.00151495\n",
      "Iteration 266, loss = 0.00149493\n",
      "Iteration 267, loss = 0.00147210\n",
      "Iteration 268, loss = 0.00147308\n",
      "Iteration 269, loss = 0.00145549\n",
      "Iteration 270, loss = 0.00142589\n",
      "Iteration 271, loss = 0.00140844\n",
      "Iteration 272, loss = 0.00139781\n",
      "Iteration 273, loss = 0.00138867\n",
      "Iteration 274, loss = 0.00139011\n",
      "Iteration 275, loss = 0.00139261\n",
      "Iteration 276, loss = 0.00138641\n",
      "Iteration 277, loss = 0.00135292\n",
      "Iteration 278, loss = 0.00135133\n",
      "Iteration 279, loss = 0.00133543\n",
      "Iteration 280, loss = 0.00132384\n",
      "Iteration 281, loss = 0.00131369\n",
      "Iteration 282, loss = 0.00131014\n",
      "Iteration 283, loss = 0.00129028\n",
      "Iteration 284, loss = 0.00128156\n",
      "Iteration 285, loss = 0.00127851\n",
      "Iteration 286, loss = 0.00126479\n",
      "Iteration 287, loss = 0.00126127\n",
      "Iteration 288, loss = 0.00125156\n",
      "Iteration 289, loss = 0.00125674\n",
      "Iteration 290, loss = 0.00125303\n",
      "Iteration 291, loss = 0.00123813\n",
      "Iteration 292, loss = 0.00121758\n",
      "Iteration 293, loss = 0.00120392\n",
      "Iteration 294, loss = 0.00119401\n",
      "Iteration 295, loss = 0.00120516\n",
      "Iteration 296, loss = 0.00117239\n",
      "Iteration 297, loss = 0.00119809\n",
      "Iteration 298, loss = 0.00115987\n",
      "Iteration 299, loss = 0.00115860\n",
      "Iteration 300, loss = 0.00114796\n",
      "Iteration 301, loss = 0.00114226\n",
      "Iteration 302, loss = 0.00113235\n",
      "Iteration 303, loss = 0.00112838\n",
      "Iteration 304, loss = 0.00111915\n",
      "Iteration 305, loss = 0.00110937\n",
      "Iteration 306, loss = 0.00111072\n",
      "Iteration 307, loss = 0.00110823\n",
      "Iteration 308, loss = 0.00110848\n",
      "Iteration 309, loss = 0.00108751\n",
      "Iteration 310, loss = 0.00106944\n",
      "Iteration 311, loss = 0.00109061\n",
      "Iteration 312, loss = 0.00106034\n",
      "Iteration 313, loss = 0.00106978\n",
      "Iteration 314, loss = 0.00105916\n",
      "Iteration 315, loss = 0.00104686\n",
      "Iteration 316, loss = 0.00103571\n",
      "Iteration 317, loss = 0.00102672\n",
      "Iteration 318, loss = 0.00102257\n",
      "Iteration 319, loss = 0.00101560\n",
      "Iteration 320, loss = 0.00101285\n",
      "Iteration 321, loss = 0.00099653\n",
      "Iteration 322, loss = 0.00100101\n",
      "Iteration 323, loss = 0.00099388\n",
      "Iteration 324, loss = 0.00098186\n",
      "Iteration 325, loss = 0.00097638\n",
      "Iteration 326, loss = 0.00097376\n",
      "Iteration 327, loss = 0.00096811\n",
      "Iteration 328, loss = 0.00095541\n",
      "Iteration 329, loss = 0.00094925\n",
      "Iteration 330, loss = 0.00094835\n",
      "Iteration 331, loss = 0.00094413\n",
      "Iteration 332, loss = 0.00093775\n",
      "Iteration 333, loss = 0.00092866\n",
      "Iteration 334, loss = 0.00092156\n",
      "Iteration 335, loss = 0.00091782\n",
      "Iteration 336, loss = 0.00091100\n",
      "Iteration 337, loss = 0.00090744\n",
      "Iteration 338, loss = 0.00091759\n",
      "Iteration 339, loss = 0.00094078\n",
      "Iteration 340, loss = 0.00089246\n",
      "Iteration 341, loss = 0.00089957\n",
      "Iteration 342, loss = 0.00087887\n",
      "Iteration 343, loss = 0.00087392\n",
      "Iteration 344, loss = 0.00086819\n",
      "Iteration 345, loss = 0.00086499\n",
      "Iteration 346, loss = 0.00085595\n",
      "Iteration 347, loss = 0.00085505\n",
      "Iteration 348, loss = 0.00084770\n",
      "Iteration 349, loss = 0.00084554\n",
      "Iteration 350, loss = 0.00083922\n",
      "Iteration 351, loss = 0.00083482\n",
      "Iteration 352, loss = 0.00083012\n",
      "Iteration 353, loss = 0.00082267\n",
      "Iteration 354, loss = 0.00082165\n",
      "Iteration 355, loss = 0.00082037\n",
      "Iteration 356, loss = 0.00081019\n",
      "Iteration 357, loss = 0.00080625\n",
      "Iteration 358, loss = 0.00080201\n",
      "Iteration 359, loss = 0.00079989\n",
      "Iteration 360, loss = 0.00079091\n",
      "Iteration 361, loss = 0.00078969\n",
      "Iteration 362, loss = 0.00078225\n",
      "Iteration 363, loss = 0.00077847\n",
      "Iteration 364, loss = 0.00077663\n",
      "Iteration 365, loss = 0.00077016\n",
      "Iteration 366, loss = 0.00076756\n",
      "Iteration 367, loss = 0.00076365\n",
      "Iteration 368, loss = 0.00076082\n",
      "Iteration 369, loss = 0.00075499\n",
      "Iteration 370, loss = 0.00075042\n",
      "Iteration 371, loss = 0.00074633\n",
      "Iteration 372, loss = 0.00074102\n",
      "Iteration 373, loss = 0.00074052\n",
      "Iteration 374, loss = 0.00073281\n",
      "Iteration 375, loss = 0.00073371\n",
      "Iteration 376, loss = 0.00072993\n",
      "Iteration 377, loss = 0.00072104\n",
      "Iteration 378, loss = 0.00072476\n",
      "Iteration 379, loss = 0.00071329\n",
      "Iteration 380, loss = 0.00071184\n",
      "Iteration 381, loss = 0.00070592\n",
      "Iteration 382, loss = 0.00070324\n",
      "Iteration 383, loss = 0.00069874\n",
      "Iteration 384, loss = 0.00069510\n",
      "Iteration 385, loss = 0.00069541\n",
      "Iteration 386, loss = 0.00069015\n",
      "Iteration 387, loss = 0.00068497\n",
      "Iteration 388, loss = 0.00068984\n",
      "Iteration 389, loss = 0.00067958\n",
      "Iteration 390, loss = 0.00068154\n",
      "Iteration 391, loss = 0.00066720\n",
      "Iteration 392, loss = 0.00066894\n",
      "Iteration 393, loss = 0.00066384\n",
      "Iteration 394, loss = 0.00065985\n",
      "Iteration 395, loss = 0.00065925\n",
      "Iteration 396, loss = 0.00065532\n",
      "Iteration 397, loss = 0.00065085\n",
      "Iteration 398, loss = 0.00064776\n",
      "Iteration 399, loss = 0.00064918\n",
      "Iteration 400, loss = 0.00064532\n",
      "Iteration 401, loss = 0.00064172\n",
      "Iteration 402, loss = 0.00063195\n",
      "Iteration 403, loss = 0.00063566\n",
      "Iteration 404, loss = 0.00062846\n",
      "Iteration 405, loss = 0.00062974\n",
      "Iteration 406, loss = 0.00062801\n",
      "Iteration 407, loss = 0.00062498\n",
      "Iteration 408, loss = 0.00061918\n",
      "Iteration 409, loss = 0.00061284\n",
      "Iteration 410, loss = 0.00061118\n",
      "Iteration 411, loss = 0.00061260\n",
      "Iteration 412, loss = 0.00060450\n",
      "Iteration 413, loss = 0.00060075\n",
      "Iteration 414, loss = 0.00060201\n",
      "Iteration 415, loss = 0.00059483\n",
      "Iteration 416, loss = 0.00059106\n",
      "Iteration 417, loss = 0.00058835\n",
      "Iteration 418, loss = 0.00058550\n",
      "Iteration 419, loss = 0.00058642\n",
      "Iteration 420, loss = 0.00058546\n",
      "Iteration 421, loss = 0.00057746\n",
      "Iteration 422, loss = 0.00057533\n",
      "Iteration 423, loss = 0.00057250\n",
      "Iteration 424, loss = 0.00057037\n",
      "Iteration 425, loss = 0.00056973\n",
      "Iteration 426, loss = 0.00056625\n",
      "Iteration 427, loss = 0.00056136\n",
      "Iteration 428, loss = 0.00055829\n",
      "Iteration 429, loss = 0.00055764\n",
      "Iteration 430, loss = 0.00055490\n",
      "Iteration 431, loss = 0.00055205\n",
      "Iteration 432, loss = 0.00055188\n",
      "Iteration 433, loss = 0.00055099\n",
      "Iteration 434, loss = 0.00054704\n",
      "Iteration 435, loss = 0.00054345\n",
      "Iteration 436, loss = 0.00053842\n",
      "Iteration 437, loss = 0.00053961\n",
      "Iteration 438, loss = 0.00053309\n",
      "Iteration 439, loss = 0.00053102\n",
      "Iteration 440, loss = 0.00052956\n",
      "Iteration 441, loss = 0.00052550\n",
      "Iteration 442, loss = 0.00052811\n",
      "Iteration 443, loss = 0.00052115\n",
      "Iteration 444, loss = 0.00052319\n",
      "Iteration 445, loss = 0.00051774\n",
      "Iteration 446, loss = 0.00051580\n",
      "Iteration 447, loss = 0.00051202\n",
      "Iteration 448, loss = 0.00051038\n",
      "Iteration 449, loss = 0.00050844\n",
      "Iteration 450, loss = 0.00050664\n",
      "Iteration 451, loss = 0.00050428\n",
      "Iteration 452, loss = 0.00050111\n",
      "Iteration 453, loss = 0.00050157\n",
      "Iteration 454, loss = 0.00049709\n",
      "Iteration 455, loss = 0.00049324\n",
      "Iteration 456, loss = 0.00049497\n",
      "Iteration 457, loss = 0.00048984\n",
      "Iteration 458, loss = 0.00048772\n",
      "Iteration 459, loss = 0.00048564\n",
      "Iteration 460, loss = 0.00048391\n",
      "Iteration 461, loss = 0.00048155\n",
      "Iteration 462, loss = 0.00047936\n",
      "Iteration 463, loss = 0.00047841\n",
      "Iteration 464, loss = 0.00047521\n",
      "Iteration 465, loss = 0.00047421\n",
      "Iteration 466, loss = 0.00047272\n",
      "Iteration 467, loss = 0.00047049\n",
      "Iteration 468, loss = 0.00046895\n",
      "Iteration 469, loss = 0.00046993\n",
      "Iteration 470, loss = 0.00046503\n",
      "Iteration 471, loss = 0.00046055\n",
      "Iteration 472, loss = 0.00046118\n",
      "Iteration 473, loss = 0.00045790\n",
      "Iteration 474, loss = 0.00045579\n",
      "Iteration 475, loss = 0.00045495\n",
      "Iteration 476, loss = 0.00045159\n",
      "Iteration 477, loss = 0.00044967\n",
      "Iteration 478, loss = 0.00045030\n",
      "Iteration 479, loss = 0.00044570\n",
      "Iteration 480, loss = 0.00044515\n",
      "Iteration 481, loss = 0.00044354\n",
      "Iteration 482, loss = 0.00044183\n",
      "Iteration 483, loss = 0.00043824\n",
      "Iteration 484, loss = 0.00043887\n",
      "Iteration 485, loss = 0.00043557\n",
      "Iteration 486, loss = 0.00043683\n",
      "Iteration 487, loss = 0.00043482\n",
      "Iteration 488, loss = 0.00043100\n",
      "Iteration 489, loss = 0.00043186\n",
      "Iteration 490, loss = 0.00043102\n",
      "Iteration 491, loss = 0.00042615\n",
      "Iteration 492, loss = 0.00042332\n",
      "Iteration 493, loss = 0.00042162\n",
      "Iteration 494, loss = 0.00042050\n",
      "Iteration 495, loss = 0.00041824\n",
      "Iteration 496, loss = 0.00041725\n",
      "Iteration 497, loss = 0.00041505\n",
      "Iteration 498, loss = 0.00041339\n",
      "Iteration 499, loss = 0.00041316\n",
      "Iteration 500, loss = 0.00041444\n",
      "Iteration 501, loss = 0.00040963\n",
      "Iteration 502, loss = 0.00040662\n",
      "Iteration 503, loss = 0.00040655\n",
      "Iteration 504, loss = 0.00040767\n",
      "Iteration 505, loss = 0.00040337\n",
      "Iteration 506, loss = 0.00040043\n",
      "Iteration 507, loss = 0.00039876\n",
      "Iteration 508, loss = 0.00039674\n",
      "Iteration 509, loss = 0.00039575\n",
      "Iteration 510, loss = 0.00039509\n",
      "Iteration 511, loss = 0.00039207\n",
      "Iteration 512, loss = 0.00039191\n",
      "Iteration 513, loss = 0.00039010\n",
      "Iteration 514, loss = 0.00038803\n",
      "Iteration 515, loss = 0.00038624\n",
      "Iteration 516, loss = 0.00038673\n",
      "Iteration 517, loss = 0.00038316\n",
      "Iteration 518, loss = 0.00038351\n",
      "Iteration 519, loss = 0.00038102\n",
      "Iteration 520, loss = 0.00037964\n",
      "Iteration 521, loss = 0.00037776\n",
      "Iteration 522, loss = 0.00037823\n",
      "Iteration 523, loss = 0.00037470\n",
      "Iteration 524, loss = 0.00037362\n",
      "Iteration 525, loss = 0.00037699\n",
      "Iteration 526, loss = 0.00036929\n",
      "Iteration 527, loss = 0.00036985\n",
      "Iteration 528, loss = 0.00036902\n",
      "Iteration 529, loss = 0.00036706\n",
      "Iteration 530, loss = 0.00036479\n",
      "Iteration 531, loss = 0.00036401\n",
      "Iteration 532, loss = 0.00036268\n",
      "Iteration 533, loss = 0.00036132\n",
      "Iteration 534, loss = 0.00036040\n",
      "Iteration 535, loss = 0.00035917\n",
      "Iteration 536, loss = 0.00035778\n",
      "Iteration 537, loss = 0.00035612\n",
      "Iteration 538, loss = 0.00035436\n",
      "Iteration 539, loss = 0.00035366\n",
      "Iteration 540, loss = 0.00035153\n",
      "Iteration 541, loss = 0.00035797\n",
      "Iteration 542, loss = 0.00034874\n",
      "Iteration 543, loss = 0.00034896\n",
      "Iteration 544, loss = 0.00034756\n",
      "Iteration 545, loss = 0.00034595\n",
      "Iteration 546, loss = 0.00034690\n",
      "Iteration 547, loss = 0.00034338\n",
      "Iteration 548, loss = 0.00034173\n",
      "Iteration 549, loss = 0.00034175\n",
      "Iteration 550, loss = 0.00033885\n",
      "Iteration 551, loss = 0.00033943\n",
      "Iteration 552, loss = 0.00033678\n",
      "Iteration 553, loss = 0.00033768\n",
      "Iteration 554, loss = 0.00033602\n",
      "Iteration 555, loss = 0.00033414\n",
      "Iteration 556, loss = 0.00033233\n",
      "Iteration 557, loss = 0.00033245\n",
      "Iteration 558, loss = 0.00033188\n",
      "Iteration 559, loss = 0.00032882\n",
      "Iteration 560, loss = 0.00032803\n",
      "Iteration 561, loss = 0.00032750\n",
      "Iteration 562, loss = 0.00032537\n",
      "Iteration 563, loss = 0.00032581\n",
      "Iteration 564, loss = 0.00032338\n",
      "Iteration 565, loss = 0.00032207\n",
      "Iteration 566, loss = 0.00032121\n",
      "Iteration 567, loss = 0.00032051\n",
      "Iteration 568, loss = 0.00031933\n",
      "Iteration 569, loss = 0.00031867\n",
      "Iteration 570, loss = 0.00031732\n",
      "Iteration 571, loss = 0.00031618\n",
      "Iteration 572, loss = 0.00031454\n",
      "Iteration 573, loss = 0.00031368\n",
      "Iteration 574, loss = 0.00031354\n",
      "Iteration 575, loss = 0.00031163\n",
      "Iteration 576, loss = 0.00031178\n",
      "Iteration 577, loss = 0.00030948\n",
      "Iteration 578, loss = 0.00030854\n",
      "Iteration 579, loss = 0.00030877\n",
      "Iteration 580, loss = 0.00030968\n",
      "Iteration 581, loss = 0.00030580\n",
      "Iteration 582, loss = 0.00030469\n",
      "Iteration 583, loss = 0.00030342\n",
      "Iteration 584, loss = 0.00030366\n",
      "Iteration 585, loss = 0.00030194\n",
      "Iteration 586, loss = 0.00030499\n",
      "Iteration 587, loss = 0.00030011\n",
      "Iteration 588, loss = 0.00029901\n",
      "Iteration 589, loss = 0.00029905\n",
      "Iteration 590, loss = 0.00029614\n",
      "Iteration 591, loss = 0.00029641\n",
      "Iteration 592, loss = 0.00029532\n",
      "Iteration 593, loss = 0.00029559\n",
      "Iteration 594, loss = 0.00029459\n",
      "Iteration 595, loss = 0.00029380\n",
      "Iteration 596, loss = 0.00029087\n",
      "Iteration 597, loss = 0.00029004\n",
      "Iteration 598, loss = 0.00028911\n",
      "Iteration 599, loss = 0.00028776\n",
      "Iteration 600, loss = 0.00028809\n",
      "Iteration 601, loss = 0.00028673\n",
      "Iteration 602, loss = 0.00028681\n",
      "Iteration 603, loss = 0.00028444\n",
      "Iteration 604, loss = 0.00028416\n",
      "Iteration 605, loss = 0.00028246\n",
      "Iteration 606, loss = 0.00028241\n",
      "Iteration 607, loss = 0.00028223\n",
      "Iteration 608, loss = 0.00028022\n",
      "Iteration 609, loss = 0.00027922\n",
      "Iteration 610, loss = 0.00027790\n",
      "Iteration 611, loss = 0.00027709\n",
      "Iteration 612, loss = 0.00027644\n",
      "Iteration 613, loss = 0.00027553\n",
      "Iteration 614, loss = 0.00027487\n",
      "Iteration 615, loss = 0.00027397\n",
      "Iteration 616, loss = 0.00027301\n",
      "Iteration 617, loss = 0.00027304\n",
      "Iteration 618, loss = 0.00027185\n",
      "Iteration 619, loss = 0.00027195\n",
      "Iteration 620, loss = 0.00027050\n",
      "Iteration 621, loss = 0.00027035\n",
      "Iteration 622, loss = 0.00026951\n",
      "Iteration 623, loss = 0.00026861\n",
      "Iteration 624, loss = 0.00026658\n",
      "Iteration 625, loss = 0.00026607\n",
      "Iteration 626, loss = 0.00026521\n",
      "Iteration 627, loss = 0.00026409\n",
      "Iteration 628, loss = 0.00026483\n",
      "Iteration 629, loss = 0.00026307\n",
      "Iteration 630, loss = 0.00026218\n",
      "Iteration 631, loss = 0.00026094\n",
      "Iteration 632, loss = 0.00026001\n",
      "Iteration 633, loss = 0.00026070\n",
      "Iteration 634, loss = 0.00025853\n",
      "Iteration 635, loss = 0.00025857\n",
      "Iteration 636, loss = 0.00025768\n",
      "Iteration 637, loss = 0.00025650\n",
      "Iteration 638, loss = 0.00025587\n",
      "Iteration 639, loss = 0.00025705\n",
      "Iteration 640, loss = 0.00025451\n",
      "Iteration 641, loss = 0.00025392\n",
      "Iteration 642, loss = 0.00025334\n",
      "Iteration 643, loss = 0.00025156\n",
      "Iteration 644, loss = 0.00025153\n",
      "Iteration 645, loss = 0.00025040\n",
      "Iteration 646, loss = 0.00025048\n",
      "Iteration 647, loss = 0.00024900\n",
      "Iteration 648, loss = 0.00024837\n",
      "Iteration 649, loss = 0.00024726\n",
      "Iteration 650, loss = 0.00024723\n",
      "Iteration 651, loss = 0.00024655\n",
      "Iteration 652, loss = 0.00024550\n",
      "Iteration 653, loss = 0.00024535\n",
      "Iteration 654, loss = 0.00024412\n",
      "Iteration 655, loss = 0.00024418\n",
      "Iteration 656, loss = 0.00024252\n",
      "Iteration 657, loss = 0.00024247\n",
      "Iteration 658, loss = 0.00024211\n",
      "Iteration 659, loss = 0.00024083\n",
      "Iteration 660, loss = 0.00024089\n",
      "Iteration 661, loss = 0.00023942\n",
      "Iteration 662, loss = 0.00023903\n",
      "Iteration 663, loss = 0.00023801\n",
      "Iteration 664, loss = 0.00023726\n",
      "Iteration 665, loss = 0.00023680\n",
      "Iteration 666, loss = 0.00023686\n",
      "Iteration 667, loss = 0.00023520\n",
      "Iteration 668, loss = 0.00023508\n",
      "Iteration 669, loss = 0.00023449\n",
      "Iteration 670, loss = 0.00023344\n",
      "Iteration 671, loss = 0.00023484\n",
      "Iteration 672, loss = 0.00023255\n",
      "Iteration 673, loss = 0.00023236\n",
      "Iteration 674, loss = 0.00023155\n",
      "Iteration 675, loss = 0.00023059\n",
      "Iteration 676, loss = 0.00022969\n",
      "Iteration 677, loss = 0.00022903\n",
      "Iteration 678, loss = 0.00022854\n",
      "Iteration 679, loss = 0.00022821\n",
      "Iteration 680, loss = 0.00022737\n",
      "Iteration 681, loss = 0.00022694\n",
      "Iteration 682, loss = 0.00022642\n",
      "Iteration 683, loss = 0.00022590\n",
      "Iteration 684, loss = 0.00022477\n",
      "Iteration 685, loss = 0.00022443\n",
      "Iteration 686, loss = 0.00022364\n",
      "Iteration 687, loss = 0.00022412\n",
      "Iteration 688, loss = 0.00022264\n",
      "Iteration 689, loss = 0.00022232\n",
      "Iteration 690, loss = 0.00022138\n",
      "Iteration 691, loss = 0.00022153\n",
      "Iteration 692, loss = 0.00022012\n",
      "Iteration 693, loss = 0.00022000\n",
      "Iteration 694, loss = 0.00021909\n",
      "Iteration 695, loss = 0.00021966\n",
      "Iteration 696, loss = 0.00021818\n",
      "Iteration 697, loss = 0.00021824\n",
      "Iteration 698, loss = 0.00021741\n",
      "Iteration 699, loss = 0.00021688\n",
      "Iteration 700, loss = 0.00021624\n",
      "Iteration 701, loss = 0.00021546\n",
      "Iteration 702, loss = 0.00021511\n",
      "Iteration 703, loss = 0.00021393\n",
      "Iteration 704, loss = 0.00021386\n",
      "Iteration 705, loss = 0.00021284\n",
      "Iteration 706, loss = 0.00021269\n",
      "Iteration 707, loss = 0.00021200\n",
      "Iteration 708, loss = 0.00021165\n",
      "Iteration 709, loss = 0.00021192\n",
      "Iteration 710, loss = 0.00021147\n",
      "Iteration 711, loss = 0.00021117\n",
      "Iteration 712, loss = 0.00020903\n",
      "Iteration 713, loss = 0.00020992\n",
      "Iteration 714, loss = 0.00020798\n",
      "Iteration 715, loss = 0.00020773\n",
      "Iteration 716, loss = 0.00020799\n",
      "Iteration 717, loss = 0.00020669\n",
      "Iteration 718, loss = 0.00020642\n",
      "Iteration 719, loss = 0.00020553\n",
      "Iteration 720, loss = 0.00020572\n",
      "Iteration 721, loss = 0.00020472\n",
      "Iteration 722, loss = 0.00020452\n",
      "Iteration 723, loss = 0.00020469\n",
      "Iteration 724, loss = 0.00020274\n",
      "Iteration 725, loss = 0.00020371\n",
      "Iteration 726, loss = 0.00020256\n",
      "Iteration 727, loss = 0.00020198\n",
      "Iteration 728, loss = 0.00020222\n",
      "Iteration 729, loss = 0.00020118\n",
      "Iteration 730, loss = 0.00020011\n",
      "Iteration 731, loss = 0.00019997\n",
      "Iteration 732, loss = 0.00019930\n",
      "Iteration 733, loss = 0.00019890\n",
      "Iteration 734, loss = 0.00019873\n",
      "Iteration 735, loss = 0.00019811\n",
      "Iteration 736, loss = 0.00019766\n",
      "Iteration 737, loss = 0.00019729\n",
      "Iteration 738, loss = 0.00019657\n",
      "Iteration 739, loss = 0.00019624\n",
      "Iteration 740, loss = 0.00019576\n",
      "Iteration 741, loss = 0.00019521\n",
      "Iteration 742, loss = 0.00019465\n",
      "Iteration 743, loss = 0.00019546\n",
      "Iteration 744, loss = 0.00019385\n",
      "Iteration 745, loss = 0.00019357\n",
      "Iteration 746, loss = 0.00019269\n",
      "Iteration 747, loss = 0.00019236\n",
      "Iteration 748, loss = 0.00019217\n",
      "Iteration 749, loss = 0.00019182\n",
      "Iteration 750, loss = 0.00019128\n",
      "Iteration 751, loss = 0.00019054\n",
      "Iteration 752, loss = 0.00019021\n",
      "Iteration 753, loss = 0.00018999\n",
      "Iteration 754, loss = 0.00018996\n",
      "Iteration 755, loss = 0.00018893\n",
      "Iteration 756, loss = 0.00018895\n",
      "Iteration 757, loss = 0.00018794\n",
      "Iteration 758, loss = 0.00018826\n",
      "Iteration 759, loss = 0.00018848\n",
      "Iteration 760, loss = 0.00018878\n",
      "Iteration 761, loss = 0.00018655\n",
      "Iteration 762, loss = 0.00018552\n",
      "Iteration 763, loss = 0.00018553\n",
      "Iteration 764, loss = 0.00018484\n",
      "Iteration 765, loss = 0.00018389\n",
      "Iteration 766, loss = 0.00018332\n",
      "Iteration 767, loss = 0.00018244\n",
      "Iteration 768, loss = 0.00018164\n",
      "Iteration 769, loss = 0.00018030\n",
      "Iteration 770, loss = 0.00017901\n",
      "Iteration 771, loss = 0.00017920\n",
      "Iteration 772, loss = 0.00017996\n",
      "Iteration 773, loss = 0.00017793\n",
      "Iteration 774, loss = 0.00017636\n",
      "Iteration 775, loss = 0.00017501\n",
      "Iteration 776, loss = 0.00017286\n",
      "Iteration 777, loss = 0.00017522\n",
      "Iteration 778, loss = 0.00017072\n",
      "Iteration 779, loss = 0.00017053\n",
      "Iteration 780, loss = 0.00017099\n",
      "Iteration 781, loss = 0.00017044\n",
      "Iteration 782, loss = 0.00016612\n",
      "Iteration 783, loss = 0.00016775\n",
      "Iteration 784, loss = 0.00016533\n",
      "Iteration 785, loss = 0.00016426\n",
      "Iteration 786, loss = 0.00016336\n",
      "Iteration 787, loss = 0.00016209\n",
      "Iteration 788, loss = 0.00016238\n",
      "Iteration 789, loss = 0.00016084\n",
      "Iteration 790, loss = 0.00016093\n",
      "Iteration 791, loss = 0.00016041\n",
      "Iteration 792, loss = 0.00015838\n",
      "Iteration 793, loss = 0.00015774\n",
      "Iteration 794, loss = 0.00015721\n",
      "Iteration 795, loss = 0.00015738\n",
      "Iteration 796, loss = 0.00015517\n",
      "Iteration 797, loss = 0.00015774\n",
      "Iteration 798, loss = 0.00015429\n",
      "Iteration 799, loss = 0.00015336\n",
      "Iteration 800, loss = 0.00015335\n",
      "Iteration 801, loss = 0.00015195\n",
      "Iteration 802, loss = 0.00015191\n",
      "Iteration 803, loss = 0.00015164\n",
      "Iteration 804, loss = 0.00015052\n",
      "Iteration 805, loss = 0.00014999\n",
      "Iteration 806, loss = 0.00014956\n",
      "Iteration 807, loss = 0.00014857\n",
      "Iteration 808, loss = 0.00014879\n",
      "Iteration 809, loss = 0.00014935\n",
      "Iteration 810, loss = 0.00014694\n",
      "Iteration 811, loss = 0.00014719\n",
      "Iteration 812, loss = 0.00014645\n",
      "Iteration 813, loss = 0.00014528\n",
      "Iteration 814, loss = 0.00014488\n",
      "Iteration 815, loss = 0.00014479\n",
      "Iteration 816, loss = 0.00014370\n",
      "Iteration 817, loss = 0.00014372\n",
      "Iteration 818, loss = 0.00014297\n",
      "Iteration 819, loss = 0.00014264\n",
      "Iteration 820, loss = 0.00014255\n",
      "Iteration 821, loss = 0.00014274\n",
      "Iteration 822, loss = 0.00014156\n",
      "Iteration 823, loss = 0.00014117\n",
      "Iteration 824, loss = 0.00014052\n",
      "Iteration 825, loss = 0.00013995\n",
      "Iteration 826, loss = 0.00013956\n",
      "Iteration 827, loss = 0.00013913\n",
      "Iteration 828, loss = 0.00013924\n",
      "Iteration 829, loss = 0.00013839\n",
      "Iteration 830, loss = 0.00013809\n",
      "Iteration 831, loss = 0.00013794\n",
      "Iteration 832, loss = 0.00013717\n",
      "Iteration 833, loss = 0.00013701\n",
      "Iteration 834, loss = 0.00013747\n",
      "Iteration 835, loss = 0.00013624\n",
      "Iteration 836, loss = 0.00013626\n",
      "Iteration 837, loss = 0.00013571\n",
      "Iteration 838, loss = 0.00013543\n",
      "Iteration 839, loss = 0.00013574\n",
      "Iteration 840, loss = 0.00013465\n",
      "Iteration 841, loss = 0.00013427\n",
      "Iteration 842, loss = 0.00013557\n",
      "Iteration 843, loss = 0.00013495\n",
      "Iteration 844, loss = 0.00013303\n",
      "Iteration 845, loss = 0.00013380\n",
      "Iteration 846, loss = 0.00013295\n",
      "Iteration 847, loss = 0.00013216\n",
      "Iteration 848, loss = 0.00013217\n",
      "Iteration 849, loss = 0.00013155\n",
      "Iteration 850, loss = 0.00013127\n",
      "Iteration 851, loss = 0.00013112\n",
      "Iteration 852, loss = 0.00013050\n",
      "Iteration 853, loss = 0.00013036\n",
      "Iteration 854, loss = 0.00013001\n",
      "Iteration 855, loss = 0.00012980\n",
      "Iteration 856, loss = 0.00012952\n",
      "Iteration 857, loss = 0.00012900\n",
      "Iteration 858, loss = 0.00012868\n",
      "Iteration 859, loss = 0.00012870\n",
      "Iteration 860, loss = 0.00012822\n",
      "Iteration 861, loss = 0.00012788\n",
      "Iteration 862, loss = 0.00012767\n",
      "Iteration 863, loss = 0.00012734\n",
      "Iteration 864, loss = 0.00012684\n",
      "Iteration 865, loss = 0.00012696\n",
      "Iteration 866, loss = 0.00012662\n",
      "Iteration 867, loss = 0.00012602\n",
      "Iteration 868, loss = 0.00012586\n",
      "Iteration 869, loss = 0.00012579\n",
      "Iteration 870, loss = 0.00012601\n",
      "Iteration 871, loss = 0.00012497\n",
      "Iteration 872, loss = 0.00012503\n",
      "Iteration 873, loss = 0.00012472\n",
      "Iteration 874, loss = 0.00012442\n",
      "Iteration 875, loss = 0.00012452\n",
      "Iteration 876, loss = 0.00012556\n",
      "Iteration 877, loss = 0.00012320\n",
      "Iteration 878, loss = 0.00012371\n",
      "Iteration 879, loss = 0.00012301\n",
      "Iteration 880, loss = 0.00012314\n",
      "Iteration 881, loss = 0.00012232\n",
      "Iteration 882, loss = 0.00012261\n",
      "Iteration 883, loss = 0.00012419\n",
      "Iteration 884, loss = 0.00012354\n",
      "Iteration 885, loss = 0.00012258\n",
      "Iteration 886, loss = 0.00012160\n",
      "Iteration 887, loss = 0.00012103\n",
      "Iteration 888, loss = 0.00012105\n",
      "Iteration 889, loss = 0.00012177\n",
      "Iteration 890, loss = 0.00011967\n",
      "Iteration 891, loss = 0.00012120\n",
      "Iteration 892, loss = 0.00012039\n",
      "Iteration 893, loss = 0.00012013\n",
      "Iteration 894, loss = 0.00011985\n",
      "Iteration 895, loss = 0.00011927\n",
      "Iteration 896, loss = 0.00011884\n",
      "Iteration 897, loss = 0.00011876\n",
      "Iteration 898, loss = 0.00011820\n",
      "Iteration 899, loss = 0.00011815\n",
      "Iteration 900, loss = 0.00011815\n",
      "Iteration 901, loss = 0.00011784\n",
      "Iteration 902, loss = 0.00011796\n",
      "Iteration 903, loss = 0.00011829\n",
      "Iteration 904, loss = 0.00011991\n",
      "Iteration 905, loss = 0.00011894\n",
      "Iteration 906, loss = 0.00011750\n",
      "Iteration 907, loss = 0.00011683\n",
      "Iteration 908, loss = 0.00011769\n",
      "Iteration 909, loss = 0.00011599\n",
      "Iteration 910, loss = 0.00011626\n",
      "Iteration 911, loss = 0.00011609\n",
      "Iteration 912, loss = 0.00011647\n",
      "Iteration 913, loss = 0.00011518\n",
      "Iteration 914, loss = 0.00011548\n",
      "Iteration 915, loss = 0.00011661\n",
      "Iteration 916, loss = 0.00011459\n",
      "Iteration 917, loss = 0.00011504\n",
      "Iteration 918, loss = 0.00011450\n",
      "Iteration 919, loss = 0.00011408\n",
      "Iteration 920, loss = 0.00011376\n",
      "Iteration 921, loss = 0.00011346\n",
      "Iteration 922, loss = 0.00011406\n",
      "Iteration 923, loss = 0.00011433\n",
      "Iteration 924, loss = 0.00011375\n",
      "Iteration 925, loss = 0.00011317\n",
      "Iteration 926, loss = 0.00011285\n",
      "Iteration 927, loss = 0.00011271\n",
      "Iteration 928, loss = 0.00011262\n",
      "Iteration 929, loss = 0.00011202\n",
      "Iteration 930, loss = 0.00011206\n",
      "Iteration 931, loss = 0.00011187\n",
      "Iteration 932, loss = 0.00011148\n",
      "Iteration 933, loss = 0.00011174\n",
      "Iteration 934, loss = 0.00011137\n",
      "Iteration 935, loss = 0.00011112\n",
      "Iteration 936, loss = 0.00011092\n",
      "Iteration 937, loss = 0.00011061\n",
      "Iteration 938, loss = 0.00011056\n",
      "Iteration 939, loss = 0.00011079\n",
      "Iteration 940, loss = 0.00011063\n",
      "Iteration 941, loss = 0.00010999\n",
      "Iteration 942, loss = 0.00010988\n",
      "Iteration 943, loss = 0.00011041\n",
      "Iteration 944, loss = 0.00011007\n",
      "Iteration 945, loss = 0.00011005\n",
      "Iteration 946, loss = 0.00010919\n",
      "Iteration 947, loss = 0.00010969\n",
      "Iteration 948, loss = 0.00010847\n",
      "Iteration 949, loss = 0.00010927\n",
      "Iteration 950, loss = 0.00010946\n",
      "Iteration 951, loss = 0.00010863\n",
      "Iteration 952, loss = 0.00010894\n",
      "Iteration 953, loss = 0.00010779\n",
      "Iteration 954, loss = 0.00010841\n",
      "Iteration 955, loss = 0.00010748\n",
      "Iteration 956, loss = 0.00010837\n",
      "Iteration 957, loss = 0.00010779\n",
      "Iteration 958, loss = 0.00010750\n",
      "Iteration 959, loss = 0.00010742\n",
      "Iteration 960, loss = 0.00010688\n",
      "Iteration 961, loss = 0.00010690\n",
      "Iteration 962, loss = 0.00010648\n",
      "Iteration 963, loss = 0.00010655\n",
      "Iteration 964, loss = 0.00010630\n",
      "Iteration 965, loss = 0.00010614\n",
      "Iteration 966, loss = 0.00010584\n",
      "Iteration 967, loss = 0.00010587\n",
      "Iteration 968, loss = 0.00010562\n",
      "Iteration 969, loss = 0.00010584\n",
      "Iteration 970, loss = 0.00010553\n",
      "Iteration 971, loss = 0.00010530\n",
      "Iteration 972, loss = 0.00010509\n",
      "Iteration 973, loss = 0.00010538\n",
      "Iteration 974, loss = 0.00010483\n",
      "Iteration 975, loss = 0.00010481\n",
      "Iteration 976, loss = 0.00010456\n",
      "Iteration 977, loss = 0.00010475\n",
      "Iteration 978, loss = 0.00010434\n",
      "Iteration 979, loss = 0.00010418\n",
      "Iteration 980, loss = 0.00010399\n",
      "Iteration 981, loss = 0.00010386\n",
      "Iteration 982, loss = 0.00010378\n",
      "Iteration 983, loss = 0.00010364\n",
      "Iteration 984, loss = 0.00010341\n",
      "Iteration 985, loss = 0.00010353\n",
      "Iteration 986, loss = 0.00010312\n",
      "Iteration 987, loss = 0.00010331\n",
      "Iteration 988, loss = 0.00010359\n",
      "Iteration 989, loss = 0.00010377\n",
      "Iteration 990, loss = 0.00010293\n",
      "Iteration 991, loss = 0.00010360\n",
      "Iteration 992, loss = 0.00010432\n",
      "Iteration 993, loss = 0.00010327\n",
      "Iteration 994, loss = 0.00010335\n",
      "Iteration 995, loss = 0.00010260\n",
      "Iteration 996, loss = 0.00010205\n",
      "Iteration 997, loss = 0.00010193\n",
      "Iteration 998, loss = 0.00010206\n",
      "Iteration 999, loss = 0.00010197\n",
      "Iteration 1000, loss = 0.00010146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "nnModelADAM = mlpADAM.fit(train_data , train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"human_activity_train.csv\")\n",
    "test = pd.read_csv(\"human_activity_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "0         -0.923527         -0.934724  ...                        -0.710304   \n",
       "1         -0.957686         -0.943068  ...                        -0.861499   \n",
       "2         -0.977469         -0.938692  ...                        -0.760104   \n",
       "3         -0.989302         -0.938692  ...                        -0.482845   \n",
       "4         -0.990441         -0.942469  ...                        -0.699205   \n",
       "\n",
       "   angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "0                    -0.112754                              0.030400   \n",
       "1                     0.053477                             -0.007435   \n",
       "2                    -0.118559                              0.177899   \n",
       "3                    -0.036788                             -0.012892   \n",
       "4                     0.123320                              0.122542   \n",
       "\n",
       "   angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "0                         -0.464761                             -0.018446   \n",
       "1                         -0.732626                              0.703511   \n",
       "2                          0.100699                              0.808529   \n",
       "3                          0.640011                             -0.485366   \n",
       "4                          0.693578                             -0.615971   \n",
       "\n",
       "   angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  subject  \\\n",
       "0             -0.841247              0.179941             -0.058627        1   \n",
       "1             -0.844788              0.180289             -0.054317        1   \n",
       "2             -0.848933              0.180637             -0.049118        1   \n",
       "3             -0.848649              0.181935             -0.047663        1   \n",
       "4             -0.847865              0.185151             -0.043892        1   \n",
       "\n",
       "   Activity  \n",
       "0  STANDING  \n",
       "1  STANDING  \n",
       "2  STANDING  \n",
       "3  STANDING  \n",
       "4  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WALKING               209\n",
       "STANDING              179\n",
       "LAYING                164\n",
       "WALKING_UPSTAIRS      159\n",
       "WALKING_DOWNSTAIRS    145\n",
       "SITTING               143\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22708d56a90>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize= (15,8))\n",
    "sns.countplot(x = 'Activity',data= train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"human_activity_train.csv\")\n",
    "test = pd.read_csv(\"human_activity_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.drop('Activity',axis = 1)\n",
    "train_label = train.Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.drop('Activity',axis = 1)\n",
    "test_label = test.Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_label)\n",
    "train_label=le.transform(train_label)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(test_label)\n",
    "test_label=le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.276673</td>\n",
       "      <td>-0.035499</td>\n",
       "      <td>-0.038827</td>\n",
       "      <td>0.120564</td>\n",
       "      <td>0.105676</td>\n",
       "      <td>-0.288769</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>0.148488</td>\n",
       "      <td>-0.330685</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597439</td>\n",
       "      <td>-0.846621</td>\n",
       "      <td>0.049025</td>\n",
       "      <td>-0.100834</td>\n",
       "      <td>-0.950340</td>\n",
       "      <td>-0.073240</td>\n",
       "      <td>-0.746880</td>\n",
       "      <td>0.237799</td>\n",
       "      <td>0.126425</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.290287</td>\n",
       "      <td>-0.017847</td>\n",
       "      <td>-0.108830</td>\n",
       "      <td>-0.982775</td>\n",
       "      <td>-0.973010</td>\n",
       "      <td>-0.985065</td>\n",
       "      <td>-0.984444</td>\n",
       "      <td>-0.979689</td>\n",
       "      <td>-0.986139</td>\n",
       "      <td>-0.920796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162046</td>\n",
       "      <td>-0.616500</td>\n",
       "      <td>0.107597</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.225829</td>\n",
       "      <td>0.283668</td>\n",
       "      <td>0.599143</td>\n",
       "      <td>-0.856702</td>\n",
       "      <td>-0.136294</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>0.318523</td>\n",
       "      <td>-0.027349</td>\n",
       "      <td>-0.110581</td>\n",
       "      <td>0.098475</td>\n",
       "      <td>0.119464</td>\n",
       "      <td>-0.347288</td>\n",
       "      <td>0.048897</td>\n",
       "      <td>-0.043545</td>\n",
       "      <td>-0.366387</td>\n",
       "      <td>0.320960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536388</td>\n",
       "      <td>0.343544</td>\n",
       "      <td>-0.627676</td>\n",
       "      <td>0.702477</td>\n",
       "      <td>0.916323</td>\n",
       "      <td>-0.682592</td>\n",
       "      <td>-0.987978</td>\n",
       "      <td>0.086802</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>0.379892</td>\n",
       "      <td>-0.014352</td>\n",
       "      <td>-0.142958</td>\n",
       "      <td>0.355264</td>\n",
       "      <td>0.281737</td>\n",
       "      <td>-0.293122</td>\n",
       "      <td>0.314118</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>-0.292639</td>\n",
       "      <td>0.587100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256430</td>\n",
       "      <td>-0.577911</td>\n",
       "      <td>-0.642957</td>\n",
       "      <td>0.821412</td>\n",
       "      <td>-0.954953</td>\n",
       "      <td>-0.885661</td>\n",
       "      <td>-0.894334</td>\n",
       "      <td>0.166254</td>\n",
       "      <td>-0.010086</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.278644</td>\n",
       "      <td>-0.012788</td>\n",
       "      <td>-0.109454</td>\n",
       "      <td>-0.989740</td>\n",
       "      <td>-0.918242</td>\n",
       "      <td>-0.984897</td>\n",
       "      <td>-0.990395</td>\n",
       "      <td>-0.905690</td>\n",
       "      <td>-0.982957</td>\n",
       "      <td>-0.932092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090760</td>\n",
       "      <td>-0.424405</td>\n",
       "      <td>0.119320</td>\n",
       "      <td>-0.056967</td>\n",
       "      <td>-0.580625</td>\n",
       "      <td>0.582217</td>\n",
       "      <td>-0.697313</td>\n",
       "      <td>0.298516</td>\n",
       "      <td>0.078604</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.275077</td>\n",
       "      <td>-0.027644</td>\n",
       "      <td>-0.064617</td>\n",
       "      <td>-0.966784</td>\n",
       "      <td>-0.959299</td>\n",
       "      <td>-0.948135</td>\n",
       "      <td>-0.977574</td>\n",
       "      <td>-0.967659</td>\n",
       "      <td>-0.946119</td>\n",
       "      <td>-0.870466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363769</td>\n",
       "      <td>-0.015948</td>\n",
       "      <td>-0.033487</td>\n",
       "      <td>0.414917</td>\n",
       "      <td>-0.271416</td>\n",
       "      <td>0.545930</td>\n",
       "      <td>0.370914</td>\n",
       "      <td>-0.665645</td>\n",
       "      <td>-0.291669</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.226680</td>\n",
       "      <td>-0.036079</td>\n",
       "      <td>-0.138726</td>\n",
       "      <td>-0.376459</td>\n",
       "      <td>0.084024</td>\n",
       "      <td>-0.300975</td>\n",
       "      <td>-0.429321</td>\n",
       "      <td>0.043470</td>\n",
       "      <td>-0.266537</td>\n",
       "      <td>-0.148724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122838</td>\n",
       "      <td>-0.443445</td>\n",
       "      <td>0.242602</td>\n",
       "      <td>-0.050322</td>\n",
       "      <td>-0.226059</td>\n",
       "      <td>-0.462972</td>\n",
       "      <td>-0.704414</td>\n",
       "      <td>0.289528</td>\n",
       "      <td>0.090365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.279183</td>\n",
       "      <td>-0.036966</td>\n",
       "      <td>-0.133604</td>\n",
       "      <td>-0.359032</td>\n",
       "      <td>-0.124112</td>\n",
       "      <td>-0.102868</td>\n",
       "      <td>-0.394230</td>\n",
       "      <td>-0.120456</td>\n",
       "      <td>-0.027792</td>\n",
       "      <td>-0.319136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230811</td>\n",
       "      <td>-0.621552</td>\n",
       "      <td>-0.183312</td>\n",
       "      <td>-0.366714</td>\n",
       "      <td>-0.932361</td>\n",
       "      <td>0.257390</td>\n",
       "      <td>-0.695481</td>\n",
       "      <td>0.303214</td>\n",
       "      <td>0.065482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.279222</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>-0.092436</td>\n",
       "      <td>-0.996173</td>\n",
       "      <td>-0.969167</td>\n",
       "      <td>-0.980864</td>\n",
       "      <td>-0.996487</td>\n",
       "      <td>-0.969335</td>\n",
       "      <td>-0.978718</td>\n",
       "      <td>-0.939277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080477</td>\n",
       "      <td>-0.185231</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>-0.255061</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>0.376215</td>\n",
       "      <td>-0.796369</td>\n",
       "      <td>0.213405</td>\n",
       "      <td>-0.065949</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>0.242757</td>\n",
       "      <td>-0.000758</td>\n",
       "      <td>-0.069354</td>\n",
       "      <td>-0.382623</td>\n",
       "      <td>0.070014</td>\n",
       "      <td>-0.478552</td>\n",
       "      <td>-0.392092</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.428664</td>\n",
       "      <td>-0.252421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>-0.338604</td>\n",
       "      <td>0.311820</td>\n",
       "      <td>-0.870191</td>\n",
       "      <td>-0.814202</td>\n",
       "      <td>-0.340962</td>\n",
       "      <td>-0.959472</td>\n",
       "      <td>0.117295</td>\n",
       "      <td>0.040097</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>0.282872</td>\n",
       "      <td>-0.015263</td>\n",
       "      <td>-0.133460</td>\n",
       "      <td>-0.978131</td>\n",
       "      <td>-0.882129</td>\n",
       "      <td>-0.940289</td>\n",
       "      <td>-0.983558</td>\n",
       "      <td>-0.874706</td>\n",
       "      <td>-0.941625</td>\n",
       "      <td>-0.909069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194594</td>\n",
       "      <td>-0.136211</td>\n",
       "      <td>-0.007073</td>\n",
       "      <td>-0.284287</td>\n",
       "      <td>-0.322336</td>\n",
       "      <td>0.614726</td>\n",
       "      <td>-0.859050</td>\n",
       "      <td>0.041942</td>\n",
       "      <td>-0.068427</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.276039</td>\n",
       "      <td>-0.017357</td>\n",
       "      <td>-0.108977</td>\n",
       "      <td>-0.996429</td>\n",
       "      <td>-0.992409</td>\n",
       "      <td>-0.992814</td>\n",
       "      <td>-0.996527</td>\n",
       "      <td>-0.991338</td>\n",
       "      <td>-0.994242</td>\n",
       "      <td>-0.941680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.647139</td>\n",
       "      <td>-0.883420</td>\n",
       "      <td>0.540635</td>\n",
       "      <td>0.441628</td>\n",
       "      <td>-0.212556</td>\n",
       "      <td>-0.498513</td>\n",
       "      <td>0.276305</td>\n",
       "      <td>-0.577603</td>\n",
       "      <td>-0.350193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.207150</td>\n",
       "      <td>-0.003435</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>-0.011734</td>\n",
       "      <td>-0.261857</td>\n",
       "      <td>-0.196767</td>\n",
       "      <td>-0.097958</td>\n",
       "      <td>-0.317471</td>\n",
       "      <td>-0.183052</td>\n",
       "      <td>0.382388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025729</td>\n",
       "      <td>-0.351122</td>\n",
       "      <td>0.533347</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>-0.409820</td>\n",
       "      <td>-0.130922</td>\n",
       "      <td>-0.742147</td>\n",
       "      <td>0.261729</td>\n",
       "      <td>0.090251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0.276219</td>\n",
       "      <td>-0.017338</td>\n",
       "      <td>-0.102674</td>\n",
       "      <td>-0.996723</td>\n",
       "      <td>-0.991117</td>\n",
       "      <td>-0.983270</td>\n",
       "      <td>-0.997237</td>\n",
       "      <td>-0.991525</td>\n",
       "      <td>-0.984055</td>\n",
       "      <td>-0.940541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200881</td>\n",
       "      <td>-0.533472</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>-0.110814</td>\n",
       "      <td>0.406559</td>\n",
       "      <td>0.088131</td>\n",
       "      <td>-0.740473</td>\n",
       "      <td>-0.049976</td>\n",
       "      <td>-0.091703</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.278316</td>\n",
       "      <td>-0.018049</td>\n",
       "      <td>-0.108403</td>\n",
       "      <td>-0.995667</td>\n",
       "      <td>-0.991870</td>\n",
       "      <td>-0.996358</td>\n",
       "      <td>-0.995761</td>\n",
       "      <td>-0.991677</td>\n",
       "      <td>-0.996794</td>\n",
       "      <td>-0.939448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523191</td>\n",
       "      <td>-0.804390</td>\n",
       "      <td>0.519163</td>\n",
       "      <td>-0.770630</td>\n",
       "      <td>-0.574786</td>\n",
       "      <td>0.844139</td>\n",
       "      <td>0.276640</td>\n",
       "      <td>-0.577824</td>\n",
       "      <td>-0.350103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.215371</td>\n",
       "      <td>-0.045300</td>\n",
       "      <td>-0.102342</td>\n",
       "      <td>-0.112943</td>\n",
       "      <td>0.147021</td>\n",
       "      <td>-0.261996</td>\n",
       "      <td>-0.199658</td>\n",
       "      <td>0.056399</td>\n",
       "      <td>-0.259647</td>\n",
       "      <td>0.221382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.439031</td>\n",
       "      <td>-0.757817</td>\n",
       "      <td>0.296343</td>\n",
       "      <td>-0.860122</td>\n",
       "      <td>0.730540</td>\n",
       "      <td>-0.315334</td>\n",
       "      <td>-0.705980</td>\n",
       "      <td>0.284190</td>\n",
       "      <td>0.100972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.276875</td>\n",
       "      <td>-0.018607</td>\n",
       "      <td>-0.112972</td>\n",
       "      <td>-0.995531</td>\n",
       "      <td>-0.977026</td>\n",
       "      <td>-0.986679</td>\n",
       "      <td>-0.996326</td>\n",
       "      <td>-0.980057</td>\n",
       "      <td>-0.985809</td>\n",
       "      <td>-0.934475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263796</td>\n",
       "      <td>-0.032810</td>\n",
       "      <td>-0.035751</td>\n",
       "      <td>-0.368238</td>\n",
       "      <td>0.466299</td>\n",
       "      <td>-0.302831</td>\n",
       "      <td>-0.681004</td>\n",
       "      <td>0.318471</td>\n",
       "      <td>0.022790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.279371</td>\n",
       "      <td>-0.017645</td>\n",
       "      <td>-0.108181</td>\n",
       "      <td>-0.995237</td>\n",
       "      <td>-0.995810</td>\n",
       "      <td>-0.994430</td>\n",
       "      <td>-0.995515</td>\n",
       "      <td>-0.995338</td>\n",
       "      <td>-0.993720</td>\n",
       "      <td>-0.939221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.672820</td>\n",
       "      <td>-0.890797</td>\n",
       "      <td>0.170203</td>\n",
       "      <td>0.310625</td>\n",
       "      <td>0.663555</td>\n",
       "      <td>0.337821</td>\n",
       "      <td>0.397791</td>\n",
       "      <td>-0.543991</td>\n",
       "      <td>-0.433791</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.288094</td>\n",
       "      <td>-0.022759</td>\n",
       "      <td>-0.135058</td>\n",
       "      <td>-0.986413</td>\n",
       "      <td>-0.992749</td>\n",
       "      <td>-0.983236</td>\n",
       "      <td>-0.986959</td>\n",
       "      <td>-0.992296</td>\n",
       "      <td>-0.983508</td>\n",
       "      <td>-0.927935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400481</td>\n",
       "      <td>-0.766425</td>\n",
       "      <td>0.044117</td>\n",
       "      <td>-0.096297</td>\n",
       "      <td>0.035648</td>\n",
       "      <td>-0.146606</td>\n",
       "      <td>0.436493</td>\n",
       "      <td>-0.814771</td>\n",
       "      <td>0.179584</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.248474</td>\n",
       "      <td>-0.037999</td>\n",
       "      <td>-0.030084</td>\n",
       "      <td>-0.949404</td>\n",
       "      <td>-0.859235</td>\n",
       "      <td>-0.786842</td>\n",
       "      <td>-0.955574</td>\n",
       "      <td>-0.873891</td>\n",
       "      <td>-0.767291</td>\n",
       "      <td>-0.865585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219153</td>\n",
       "      <td>-0.188160</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>-0.421327</td>\n",
       "      <td>-0.373610</td>\n",
       "      <td>-0.502968</td>\n",
       "      <td>-0.744851</td>\n",
       "      <td>0.262502</td>\n",
       "      <td>-0.047606</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.281794</td>\n",
       "      <td>-0.014159</td>\n",
       "      <td>-0.098780</td>\n",
       "      <td>-0.953150</td>\n",
       "      <td>-0.899854</td>\n",
       "      <td>-0.959204</td>\n",
       "      <td>-0.961445</td>\n",
       "      <td>-0.900192</td>\n",
       "      <td>-0.959443</td>\n",
       "      <td>-0.862438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137299</td>\n",
       "      <td>-0.515811</td>\n",
       "      <td>-0.064355</td>\n",
       "      <td>0.030715</td>\n",
       "      <td>-0.807828</td>\n",
       "      <td>0.489175</td>\n",
       "      <td>-0.750181</td>\n",
       "      <td>0.270219</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.263123</td>\n",
       "      <td>-0.022800</td>\n",
       "      <td>-0.090274</td>\n",
       "      <td>-0.991619</td>\n",
       "      <td>-0.989278</td>\n",
       "      <td>-0.992551</td>\n",
       "      <td>-0.993212</td>\n",
       "      <td>-0.989961</td>\n",
       "      <td>-0.992815</td>\n",
       "      <td>-0.931332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505631</td>\n",
       "      <td>-0.855800</td>\n",
       "      <td>0.067526</td>\n",
       "      <td>-0.104690</td>\n",
       "      <td>0.046823</td>\n",
       "      <td>0.389168</td>\n",
       "      <td>0.372217</td>\n",
       "      <td>-0.654501</td>\n",
       "      <td>-0.304835</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0.223498</td>\n",
       "      <td>-0.004764</td>\n",
       "      <td>-0.054825</td>\n",
       "      <td>-0.298798</td>\n",
       "      <td>-0.115901</td>\n",
       "      <td>-0.317805</td>\n",
       "      <td>-0.336247</td>\n",
       "      <td>-0.124901</td>\n",
       "      <td>-0.333654</td>\n",
       "      <td>0.022884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550369</td>\n",
       "      <td>0.344301</td>\n",
       "      <td>0.476859</td>\n",
       "      <td>0.083179</td>\n",
       "      <td>-0.093855</td>\n",
       "      <td>-0.775732</td>\n",
       "      <td>-0.721141</td>\n",
       "      <td>0.265173</td>\n",
       "      <td>0.116227</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.276191</td>\n",
       "      <td>-0.015569</td>\n",
       "      <td>-0.118808</td>\n",
       "      <td>-0.992043</td>\n",
       "      <td>-0.935871</td>\n",
       "      <td>-0.970479</td>\n",
       "      <td>-0.994291</td>\n",
       "      <td>-0.934291</td>\n",
       "      <td>-0.971623</td>\n",
       "      <td>-0.926832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057120</td>\n",
       "      <td>-0.275847</td>\n",
       "      <td>0.079812</td>\n",
       "      <td>-0.049976</td>\n",
       "      <td>0.833767</td>\n",
       "      <td>-0.585575</td>\n",
       "      <td>-0.745210</td>\n",
       "      <td>0.273560</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.208768</td>\n",
       "      <td>0.030679</td>\n",
       "      <td>0.037289</td>\n",
       "      <td>0.215876</td>\n",
       "      <td>0.092032</td>\n",
       "      <td>-0.296456</td>\n",
       "      <td>0.116780</td>\n",
       "      <td>-0.002253</td>\n",
       "      <td>-0.281541</td>\n",
       "      <td>0.537740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.570777</td>\n",
       "      <td>-0.861041</td>\n",
       "      <td>0.325617</td>\n",
       "      <td>-0.675525</td>\n",
       "      <td>-0.964238</td>\n",
       "      <td>0.664562</td>\n",
       "      <td>-0.684240</td>\n",
       "      <td>0.295148</td>\n",
       "      <td>0.113016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.278457</td>\n",
       "      <td>-0.020415</td>\n",
       "      <td>-0.112732</td>\n",
       "      <td>-0.999135</td>\n",
       "      <td>-0.984680</td>\n",
       "      <td>-0.996274</td>\n",
       "      <td>-0.999077</td>\n",
       "      <td>-0.982937</td>\n",
       "      <td>-0.996410</td>\n",
       "      <td>-0.943906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.686389</td>\n",
       "      <td>-0.878751</td>\n",
       "      <td>-0.077552</td>\n",
       "      <td>-0.101222</td>\n",
       "      <td>0.639084</td>\n",
       "      <td>0.765485</td>\n",
       "      <td>-0.850654</td>\n",
       "      <td>0.187611</td>\n",
       "      <td>-0.035998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.356419</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>-0.094958</td>\n",
       "      <td>-0.334456</td>\n",
       "      <td>-0.078653</td>\n",
       "      <td>-0.550542</td>\n",
       "      <td>-0.338711</td>\n",
       "      <td>-0.117800</td>\n",
       "      <td>-0.514075</td>\n",
       "      <td>-0.252745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161945</td>\n",
       "      <td>-0.539289</td>\n",
       "      <td>-0.598557</td>\n",
       "      <td>-0.041629</td>\n",
       "      <td>0.624940</td>\n",
       "      <td>-0.601020</td>\n",
       "      <td>-0.964063</td>\n",
       "      <td>0.116167</td>\n",
       "      <td>0.036567</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.298554</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>-0.040696</td>\n",
       "      <td>-0.198107</td>\n",
       "      <td>-0.004638</td>\n",
       "      <td>-0.185038</td>\n",
       "      <td>-0.246645</td>\n",
       "      <td>-0.062917</td>\n",
       "      <td>-0.139328</td>\n",
       "      <td>-0.019153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395030</td>\n",
       "      <td>-0.649193</td>\n",
       "      <td>-0.035510</td>\n",
       "      <td>-0.760585</td>\n",
       "      <td>0.535230</td>\n",
       "      <td>-0.957050</td>\n",
       "      <td>-0.773199</td>\n",
       "      <td>0.254039</td>\n",
       "      <td>0.034529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.278123</td>\n",
       "      <td>-0.017146</td>\n",
       "      <td>-0.108014</td>\n",
       "      <td>-0.994928</td>\n",
       "      <td>-0.998359</td>\n",
       "      <td>-0.992011</td>\n",
       "      <td>-0.995586</td>\n",
       "      <td>-0.998077</td>\n",
       "      <td>-0.992591</td>\n",
       "      <td>-0.938974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900519</td>\n",
       "      <td>-0.988448</td>\n",
       "      <td>-0.327661</td>\n",
       "      <td>-0.689303</td>\n",
       "      <td>0.397035</td>\n",
       "      <td>-0.310911</td>\n",
       "      <td>0.398897</td>\n",
       "      <td>-0.546859</td>\n",
       "      <td>-0.431054</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.287782</td>\n",
       "      <td>-0.020286</td>\n",
       "      <td>-0.102245</td>\n",
       "      <td>-0.980346</td>\n",
       "      <td>-0.935654</td>\n",
       "      <td>-0.966177</td>\n",
       "      <td>-0.982792</td>\n",
       "      <td>-0.937887</td>\n",
       "      <td>-0.966048</td>\n",
       "      <td>-0.911852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402740</td>\n",
       "      <td>-0.701041</td>\n",
       "      <td>-0.027746</td>\n",
       "      <td>0.132856</td>\n",
       "      <td>-0.113760</td>\n",
       "      <td>-0.227883</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>-0.531092</td>\n",
       "      <td>-0.465743</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>0.283391</td>\n",
       "      <td>-0.017404</td>\n",
       "      <td>-0.106603</td>\n",
       "      <td>-0.987043</td>\n",
       "      <td>-0.987328</td>\n",
       "      <td>-0.984720</td>\n",
       "      <td>-0.987481</td>\n",
       "      <td>-0.986007</td>\n",
       "      <td>-0.987248</td>\n",
       "      <td>-0.934684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.510066</td>\n",
       "      <td>-0.871327</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>-0.288375</td>\n",
       "      <td>0.263906</td>\n",
       "      <td>-0.669137</td>\n",
       "      <td>0.602412</td>\n",
       "      <td>-0.855684</td>\n",
       "      <td>-0.137260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0.335911</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>-0.148065</td>\n",
       "      <td>-0.191444</td>\n",
       "      <td>-0.155987</td>\n",
       "      <td>-0.358816</td>\n",
       "      <td>-0.275245</td>\n",
       "      <td>-0.242165</td>\n",
       "      <td>-0.366597</td>\n",
       "      <td>0.245572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.378104</td>\n",
       "      <td>-0.703402</td>\n",
       "      <td>-0.337421</td>\n",
       "      <td>0.695591</td>\n",
       "      <td>0.700064</td>\n",
       "      <td>-0.655065</td>\n",
       "      <td>-0.807519</td>\n",
       "      <td>0.225151</td>\n",
       "      <td>0.058854</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.202011</td>\n",
       "      <td>-0.009038</td>\n",
       "      <td>-0.079096</td>\n",
       "      <td>-0.220770</td>\n",
       "      <td>0.123136</td>\n",
       "      <td>-0.221638</td>\n",
       "      <td>-0.271588</td>\n",
       "      <td>0.072742</td>\n",
       "      <td>-0.210427</td>\n",
       "      <td>-0.119485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000361</td>\n",
       "      <td>-0.412768</td>\n",
       "      <td>0.684841</td>\n",
       "      <td>0.424516</td>\n",
       "      <td>-0.284663</td>\n",
       "      <td>-0.021918</td>\n",
       "      <td>-0.766180</td>\n",
       "      <td>0.258603</td>\n",
       "      <td>0.037376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.180479</td>\n",
       "      <td>-0.042536</td>\n",
       "      <td>-0.280561</td>\n",
       "      <td>-0.956753</td>\n",
       "      <td>-0.939516</td>\n",
       "      <td>-0.763049</td>\n",
       "      <td>-0.956820</td>\n",
       "      <td>-0.931894</td>\n",
       "      <td>-0.738179</td>\n",
       "      <td>-0.933911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026643</td>\n",
       "      <td>-0.351440</td>\n",
       "      <td>0.008330</td>\n",
       "      <td>-0.398115</td>\n",
       "      <td>-0.161538</td>\n",
       "      <td>0.146414</td>\n",
       "      <td>0.444490</td>\n",
       "      <td>-0.786322</td>\n",
       "      <td>0.217390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.019016</td>\n",
       "      <td>-0.007037</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.661294</td>\n",
       "      <td>-0.713353</td>\n",
       "      <td>-0.701155</td>\n",
       "      <td>-0.693948</td>\n",
       "      <td>-0.706558</td>\n",
       "      <td>-0.755422</td>\n",
       "      <td>-0.891284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396355</td>\n",
       "      <td>0.070405</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>0.061568</td>\n",
       "      <td>-0.514438</td>\n",
       "      <td>0.240943</td>\n",
       "      <td>0.205759</td>\n",
       "      <td>-0.536104</td>\n",
       "      <td>-0.360736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>0.427562</td>\n",
       "      <td>-0.044763</td>\n",
       "      <td>-0.155940</td>\n",
       "      <td>-0.034277</td>\n",
       "      <td>-0.067031</td>\n",
       "      <td>-0.352049</td>\n",
       "      <td>-0.120583</td>\n",
       "      <td>-0.043077</td>\n",
       "      <td>-0.341470</td>\n",
       "      <td>0.463206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257050</td>\n",
       "      <td>-0.670590</td>\n",
       "      <td>-0.771521</td>\n",
       "      <td>-0.898913</td>\n",
       "      <td>0.948149</td>\n",
       "      <td>-0.420604</td>\n",
       "      <td>-0.734896</td>\n",
       "      <td>0.253871</td>\n",
       "      <td>0.116931</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.280586</td>\n",
       "      <td>-0.009960</td>\n",
       "      <td>-0.106065</td>\n",
       "      <td>-0.994803</td>\n",
       "      <td>-0.972758</td>\n",
       "      <td>-0.986244</td>\n",
       "      <td>-0.995405</td>\n",
       "      <td>-0.973663</td>\n",
       "      <td>-0.985642</td>\n",
       "      <td>-0.940028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339526</td>\n",
       "      <td>0.140452</td>\n",
       "      <td>-0.020590</td>\n",
       "      <td>-0.127730</td>\n",
       "      <td>-0.482871</td>\n",
       "      <td>-0.070670</td>\n",
       "      <td>-0.848294</td>\n",
       "      <td>0.190310</td>\n",
       "      <td>-0.034417</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.158517</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>-0.018779</td>\n",
       "      <td>-0.406022</td>\n",
       "      <td>-0.083614</td>\n",
       "      <td>-0.012498</td>\n",
       "      <td>-0.466763</td>\n",
       "      <td>-0.101567</td>\n",
       "      <td>0.094817</td>\n",
       "      <td>-0.232947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522033</td>\n",
       "      <td>-0.821995</td>\n",
       "      <td>0.498854</td>\n",
       "      <td>0.179758</td>\n",
       "      <td>-0.818937</td>\n",
       "      <td>-0.056916</td>\n",
       "      <td>-0.698389</td>\n",
       "      <td>0.304878</td>\n",
       "      <td>0.044325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0.274242</td>\n",
       "      <td>-0.016038</td>\n",
       "      <td>-0.112770</td>\n",
       "      <td>-0.994767</td>\n",
       "      <td>-0.993026</td>\n",
       "      <td>-0.995245</td>\n",
       "      <td>-0.995300</td>\n",
       "      <td>-0.993950</td>\n",
       "      <td>-0.995172</td>\n",
       "      <td>-0.939363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762665</td>\n",
       "      <td>-0.879145</td>\n",
       "      <td>0.098904</td>\n",
       "      <td>0.768345</td>\n",
       "      <td>0.281229</td>\n",
       "      <td>0.399057</td>\n",
       "      <td>0.453743</td>\n",
       "      <td>-0.661493</td>\n",
       "      <td>-0.322760</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.279553</td>\n",
       "      <td>-0.017001</td>\n",
       "      <td>-0.110517</td>\n",
       "      <td>-0.996076</td>\n",
       "      <td>-0.995782</td>\n",
       "      <td>-0.998222</td>\n",
       "      <td>-0.996287</td>\n",
       "      <td>-0.996370</td>\n",
       "      <td>-0.999036</td>\n",
       "      <td>-0.941617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537470</td>\n",
       "      <td>-0.861926</td>\n",
       "      <td>-0.008144</td>\n",
       "      <td>0.559318</td>\n",
       "      <td>0.142527</td>\n",
       "      <td>-0.016025</td>\n",
       "      <td>-0.314575</td>\n",
       "      <td>-0.218935</td>\n",
       "      <td>-0.323988</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.289309</td>\n",
       "      <td>-0.039572</td>\n",
       "      <td>-0.120292</td>\n",
       "      <td>-0.333353</td>\n",
       "      <td>0.053256</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>-0.380100</td>\n",
       "      <td>0.104575</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>-0.161963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.526320</td>\n",
       "      <td>-0.820887</td>\n",
       "      <td>-0.362818</td>\n",
       "      <td>-0.720317</td>\n",
       "      <td>-0.949112</td>\n",
       "      <td>-0.691951</td>\n",
       "      <td>-0.613925</td>\n",
       "      <td>0.350310</td>\n",
       "      <td>0.104406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>0.268044</td>\n",
       "      <td>-0.014861</td>\n",
       "      <td>-0.105414</td>\n",
       "      <td>-0.990687</td>\n",
       "      <td>-0.976967</td>\n",
       "      <td>-0.981729</td>\n",
       "      <td>-0.991500</td>\n",
       "      <td>-0.977235</td>\n",
       "      <td>-0.981998</td>\n",
       "      <td>-0.938679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.484983</td>\n",
       "      <td>-0.842352</td>\n",
       "      <td>0.190303</td>\n",
       "      <td>0.223932</td>\n",
       "      <td>-0.007977</td>\n",
       "      <td>-0.050694</td>\n",
       "      <td>-0.363524</td>\n",
       "      <td>-0.289190</td>\n",
       "      <td>-0.182467</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.153296</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>-0.808638</td>\n",
       "      <td>-0.572746</td>\n",
       "      <td>-0.626415</td>\n",
       "      <td>-0.806213</td>\n",
       "      <td>-0.533811</td>\n",
       "      <td>-0.579394</td>\n",
       "      <td>-0.916064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143358</td>\n",
       "      <td>-0.097418</td>\n",
       "      <td>-0.004913</td>\n",
       "      <td>0.033988</td>\n",
       "      <td>-0.036068</td>\n",
       "      <td>0.205113</td>\n",
       "      <td>-0.418368</td>\n",
       "      <td>-0.142549</td>\n",
       "      <td>-0.305884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.277152</td>\n",
       "      <td>-0.017983</td>\n",
       "      <td>-0.106601</td>\n",
       "      <td>-0.997763</td>\n",
       "      <td>-0.989957</td>\n",
       "      <td>-0.996586</td>\n",
       "      <td>-0.998291</td>\n",
       "      <td>-0.989669</td>\n",
       "      <td>-0.996700</td>\n",
       "      <td>-0.941472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.848198</td>\n",
       "      <td>-0.950247</td>\n",
       "      <td>-0.002320</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.142331</td>\n",
       "      <td>-0.853711</td>\n",
       "      <td>-0.762023</td>\n",
       "      <td>0.262170</td>\n",
       "      <td>0.029987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.231799</td>\n",
       "      <td>-0.007522</td>\n",
       "      <td>-0.055801</td>\n",
       "      <td>0.063689</td>\n",
       "      <td>-0.017155</td>\n",
       "      <td>-0.230358</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>-0.073383</td>\n",
       "      <td>-0.182779</td>\n",
       "      <td>0.460332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251705</td>\n",
       "      <td>-0.668827</td>\n",
       "      <td>0.378196</td>\n",
       "      <td>0.894064</td>\n",
       "      <td>0.812622</td>\n",
       "      <td>-0.424187</td>\n",
       "      <td>-0.750713</td>\n",
       "      <td>0.267236</td>\n",
       "      <td>0.051523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.294121</td>\n",
       "      <td>-0.015314</td>\n",
       "      <td>-0.111396</td>\n",
       "      <td>-0.991548</td>\n",
       "      <td>-0.964032</td>\n",
       "      <td>-0.980105</td>\n",
       "      <td>-0.992227</td>\n",
       "      <td>-0.963608</td>\n",
       "      <td>-0.980810</td>\n",
       "      <td>-0.930330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539127</td>\n",
       "      <td>-0.844971</td>\n",
       "      <td>-0.078489</td>\n",
       "      <td>0.528527</td>\n",
       "      <td>0.349670</td>\n",
       "      <td>-0.171902</td>\n",
       "      <td>0.482282</td>\n",
       "      <td>-0.530567</td>\n",
       "      <td>-0.466610</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.286568</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>-0.092536</td>\n",
       "      <td>-0.358399</td>\n",
       "      <td>0.184799</td>\n",
       "      <td>-0.360064</td>\n",
       "      <td>-0.421587</td>\n",
       "      <td>0.095438</td>\n",
       "      <td>-0.358339</td>\n",
       "      <td>-0.257466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088814</td>\n",
       "      <td>-0.462908</td>\n",
       "      <td>0.146485</td>\n",
       "      <td>-0.690094</td>\n",
       "      <td>-0.886370</td>\n",
       "      <td>0.500656</td>\n",
       "      <td>-0.687830</td>\n",
       "      <td>0.299898</td>\n",
       "      <td>0.095160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.289727</td>\n",
       "      <td>0.015343</td>\n",
       "      <td>-0.116675</td>\n",
       "      <td>-0.368800</td>\n",
       "      <td>-0.083343</td>\n",
       "      <td>-0.263347</td>\n",
       "      <td>-0.423650</td>\n",
       "      <td>-0.178759</td>\n",
       "      <td>-0.277213</td>\n",
       "      <td>-0.193089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180428</td>\n",
       "      <td>-0.542152</td>\n",
       "      <td>0.054138</td>\n",
       "      <td>-0.492607</td>\n",
       "      <td>0.615095</td>\n",
       "      <td>0.346940</td>\n",
       "      <td>-0.774100</td>\n",
       "      <td>0.253689</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.310360</td>\n",
       "      <td>-0.054635</td>\n",
       "      <td>-0.139121</td>\n",
       "      <td>-0.299196</td>\n",
       "      <td>0.131415</td>\n",
       "      <td>-0.311024</td>\n",
       "      <td>-0.395266</td>\n",
       "      <td>0.088386</td>\n",
       "      <td>-0.302162</td>\n",
       "      <td>-0.104191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215522</td>\n",
       "      <td>-0.501653</td>\n",
       "      <td>-0.365812</td>\n",
       "      <td>0.805359</td>\n",
       "      <td>0.323233</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>-0.711755</td>\n",
       "      <td>0.285024</td>\n",
       "      <td>0.087985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.265630</td>\n",
       "      <td>-0.018372</td>\n",
       "      <td>-0.101204</td>\n",
       "      <td>-0.992917</td>\n",
       "      <td>-0.988180</td>\n",
       "      <td>-0.989387</td>\n",
       "      <td>-0.993344</td>\n",
       "      <td>-0.988862</td>\n",
       "      <td>-0.989551</td>\n",
       "      <td>-0.945556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>-0.275918</td>\n",
       "      <td>-0.018798</td>\n",
       "      <td>-0.055183</td>\n",
       "      <td>-0.597092</td>\n",
       "      <td>0.556851</td>\n",
       "      <td>0.451020</td>\n",
       "      <td>-0.663451</td>\n",
       "      <td>-0.319994</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>0.277588</td>\n",
       "      <td>-0.026988</td>\n",
       "      <td>-0.098282</td>\n",
       "      <td>-0.996137</td>\n",
       "      <td>-0.965252</td>\n",
       "      <td>-0.973905</td>\n",
       "      <td>-0.996368</td>\n",
       "      <td>-0.964849</td>\n",
       "      <td>-0.974186</td>\n",
       "      <td>-0.940426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108549</td>\n",
       "      <td>-0.450341</td>\n",
       "      <td>0.043198</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>-0.069545</td>\n",
       "      <td>-0.128496</td>\n",
       "      <td>-0.935384</td>\n",
       "      <td>0.074459</td>\n",
       "      <td>0.060654</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.179845</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>-0.113524</td>\n",
       "      <td>-0.409452</td>\n",
       "      <td>-0.124976</td>\n",
       "      <td>-0.396023</td>\n",
       "      <td>-0.483295</td>\n",
       "      <td>-0.153687</td>\n",
       "      <td>-0.349319</td>\n",
       "      <td>-0.254782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.525978</td>\n",
       "      <td>-0.819203</td>\n",
       "      <td>0.866832</td>\n",
       "      <td>0.357184</td>\n",
       "      <td>0.909559</td>\n",
       "      <td>-0.750959</td>\n",
       "      <td>-0.615279</td>\n",
       "      <td>0.343461</td>\n",
       "      <td>0.120002</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.196957</td>\n",
       "      <td>0.056390</td>\n",
       "      <td>0.121614</td>\n",
       "      <td>-0.815371</td>\n",
       "      <td>-0.718973</td>\n",
       "      <td>-0.585037</td>\n",
       "      <td>-0.857613</td>\n",
       "      <td>-0.710190</td>\n",
       "      <td>-0.546371</td>\n",
       "      <td>-0.620883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071829</td>\n",
       "      <td>-0.536386</td>\n",
       "      <td>0.051518</td>\n",
       "      <td>0.535813</td>\n",
       "      <td>0.044502</td>\n",
       "      <td>0.019167</td>\n",
       "      <td>-0.849254</td>\n",
       "      <td>0.114041</td>\n",
       "      <td>-0.090878</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.181817</td>\n",
       "      <td>-0.026175</td>\n",
       "      <td>-0.057208</td>\n",
       "      <td>-0.317952</td>\n",
       "      <td>-0.105712</td>\n",
       "      <td>-0.338104</td>\n",
       "      <td>-0.367994</td>\n",
       "      <td>-0.174597</td>\n",
       "      <td>-0.355963</td>\n",
       "      <td>-0.064369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351741</td>\n",
       "      <td>0.070033</td>\n",
       "      <td>0.577947</td>\n",
       "      <td>0.172898</td>\n",
       "      <td>-0.089264</td>\n",
       "      <td>-0.211844</td>\n",
       "      <td>-0.719842</td>\n",
       "      <td>0.256581</td>\n",
       "      <td>0.131992</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.286857</td>\n",
       "      <td>-0.022728</td>\n",
       "      <td>-0.143757</td>\n",
       "      <td>-0.990860</td>\n",
       "      <td>-0.962100</td>\n",
       "      <td>-0.968136</td>\n",
       "      <td>-0.991492</td>\n",
       "      <td>-0.968516</td>\n",
       "      <td>-0.967290</td>\n",
       "      <td>-0.929766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095466</td>\n",
       "      <td>-0.580139</td>\n",
       "      <td>-0.021361</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>-0.217529</td>\n",
       "      <td>0.401879</td>\n",
       "      <td>-0.866386</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>-0.049782</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.239824</td>\n",
       "      <td>-0.012820</td>\n",
       "      <td>-0.097670</td>\n",
       "      <td>-0.977652</td>\n",
       "      <td>-0.979160</td>\n",
       "      <td>-0.988065</td>\n",
       "      <td>-0.978853</td>\n",
       "      <td>-0.978020</td>\n",
       "      <td>-0.989085</td>\n",
       "      <td>-0.928237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595447</td>\n",
       "      <td>-0.870533</td>\n",
       "      <td>-0.171237</td>\n",
       "      <td>0.015460</td>\n",
       "      <td>-0.322279</td>\n",
       "      <td>0.349761</td>\n",
       "      <td>0.440432</td>\n",
       "      <td>-0.659452</td>\n",
       "      <td>-0.321601</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0.311377</td>\n",
       "      <td>-0.025185</td>\n",
       "      <td>-0.147144</td>\n",
       "      <td>-0.298204</td>\n",
       "      <td>-0.105104</td>\n",
       "      <td>-0.372639</td>\n",
       "      <td>-0.383011</td>\n",
       "      <td>-0.145947</td>\n",
       "      <td>-0.360756</td>\n",
       "      <td>0.069007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374946</td>\n",
       "      <td>0.065585</td>\n",
       "      <td>-0.448352</td>\n",
       "      <td>0.758061</td>\n",
       "      <td>0.941960</td>\n",
       "      <td>-0.677067</td>\n",
       "      <td>-0.618231</td>\n",
       "      <td>0.335930</td>\n",
       "      <td>0.132129</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.041701</td>\n",
       "      <td>0.175102</td>\n",
       "      <td>0.025552</td>\n",
       "      <td>-0.758393</td>\n",
       "      <td>-0.586537</td>\n",
       "      <td>-0.439110</td>\n",
       "      <td>-0.774118</td>\n",
       "      <td>-0.555350</td>\n",
       "      <td>-0.438736</td>\n",
       "      <td>-0.737152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482485</td>\n",
       "      <td>0.256095</td>\n",
       "      <td>0.114760</td>\n",
       "      <td>0.114436</td>\n",
       "      <td>-0.070782</td>\n",
       "      <td>-0.001461</td>\n",
       "      <td>-0.552939</td>\n",
       "      <td>-0.053539</td>\n",
       "      <td>-0.260424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.325974</td>\n",
       "      <td>-0.011532</td>\n",
       "      <td>-0.085330</td>\n",
       "      <td>-0.414957</td>\n",
       "      <td>-0.137715</td>\n",
       "      <td>-0.447530</td>\n",
       "      <td>-0.495048</td>\n",
       "      <td>-0.139762</td>\n",
       "      <td>-0.429071</td>\n",
       "      <td>-0.068281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352189</td>\n",
       "      <td>-0.665037</td>\n",
       "      <td>-0.374908</td>\n",
       "      <td>-0.392331</td>\n",
       "      <td>0.405123</td>\n",
       "      <td>0.777508</td>\n",
       "      <td>-0.615527</td>\n",
       "      <td>0.344476</td>\n",
       "      <td>0.117040</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.361205</td>\n",
       "      <td>-0.268121</td>\n",
       "      <td>0.176896</td>\n",
       "      <td>-0.599392</td>\n",
       "      <td>0.532506</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>-0.664108</td>\n",
       "      <td>0.502260</td>\n",
       "      <td>0.048472</td>\n",
       "      <td>-0.717081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063364</td>\n",
       "      <td>-0.330520</td>\n",
       "      <td>0.152830</td>\n",
       "      <td>0.149602</td>\n",
       "      <td>0.127245</td>\n",
       "      <td>0.192852</td>\n",
       "      <td>0.508733</td>\n",
       "      <td>-0.496132</td>\n",
       "      <td>-0.506197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows Ã— 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "662           0.276673          -0.035499          -0.038827   \n",
       "766           0.290287          -0.017847          -0.108830   \n",
       "802           0.318523          -0.027349          -0.110581   \n",
       "959           0.379892          -0.014352          -0.142958   \n",
       "551           0.278644          -0.012788          -0.109454   \n",
       "596           0.275077          -0.027644          -0.064617   \n",
       "253           0.226680          -0.036079          -0.138726   \n",
       "151           0.279183          -0.036966          -0.133604   \n",
       "994           0.279222          -0.005795          -0.092436   \n",
       "794           0.242757          -0.000758          -0.069354   \n",
       "733           0.282872          -0.015263          -0.133460   \n",
       "220           0.276039          -0.017357          -0.108977   \n",
       "297           0.207150          -0.003435          -0.143308   \n",
       "884           0.276219          -0.017338          -0.102674   \n",
       "221           0.278316          -0.018049          -0.108403   \n",
       "335           0.215371          -0.045300          -0.102342   \n",
       "185           0.276875          -0.018607          -0.112972   \n",
       "63            0.279371          -0.017645          -0.108181   \n",
       "243           0.288094          -0.022759          -0.135058   \n",
       "358           0.248474          -0.037999          -0.030084   \n",
       "525           0.281794          -0.014159          -0.098780   \n",
       "597           0.263123          -0.022800          -0.090274   \n",
       "456           0.223498          -0.004764          -0.054825   \n",
       "528           0.276191          -0.015569          -0.118808   \n",
       "330           0.208768           0.030679           0.037289   \n",
       "12            0.278457          -0.020415          -0.112732   \n",
       "795           0.356419           0.001897          -0.094958   \n",
       "105           0.298554           0.000794          -0.040696   \n",
       "56            0.278123          -0.017146          -0.108014   \n",
       "412           0.287782          -0.020286          -0.102245   \n",
       "..                 ...                ...                ...   \n",
       "758           0.283391          -0.017404          -0.106603   \n",
       "637           0.335911           0.003170          -0.148065   \n",
       "115           0.202011          -0.009038          -0.079096   \n",
       "246           0.180479          -0.042536          -0.280561   \n",
       "64            0.019016          -0.007037          -0.028333   \n",
       "656           0.427562          -0.044763          -0.155940   \n",
       "9             0.280586          -0.009960          -0.106065   \n",
       "155           0.158517           0.004256          -0.018779   \n",
       "433           0.274242          -0.016038          -0.112770   \n",
       "36            0.279553          -0.017001          -0.110517   \n",
       "305           0.289309          -0.039572          -0.120292   \n",
       "721           0.268044          -0.014861          -0.105414   \n",
       "32            0.013904           0.153296           0.016243   \n",
       "21            0.277152          -0.017983          -0.106601   \n",
       "139           0.231799          -0.007522          -0.055801   \n",
       "409           0.294121          -0.015314          -0.111396   \n",
       "276           0.286568           0.020619          -0.092536   \n",
       "120           0.289727           0.015343          -0.116675   \n",
       "278           0.310360          -0.054635          -0.139121   \n",
       "430           0.265630          -0.018372          -0.101204   \n",
       "574           0.277588          -0.026988          -0.098282   \n",
       "493           0.179845           0.000379          -0.113524   \n",
       "688           0.196957           0.056390           0.121614   \n",
       "618           0.181817          -0.026175          -0.057208   \n",
       "398           0.286857          -0.022728          -0.143757   \n",
       "419           0.239824          -0.012820          -0.097670   \n",
       "670           0.311377          -0.025185          -0.147144   \n",
       "31           -0.041701           0.175102           0.025552   \n",
       "492           0.325974          -0.011532          -0.085330   \n",
       "70           -0.361205          -0.268121           0.176896   \n",
       "\n",
       "     tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "662          0.120564          0.105676         -0.288769          0.052222   \n",
       "766         -0.982775         -0.973010         -0.985065         -0.984444   \n",
       "802          0.098475          0.119464         -0.347288          0.048897   \n",
       "959          0.355264          0.281737         -0.293122          0.314118   \n",
       "551         -0.989740         -0.918242         -0.984897         -0.990395   \n",
       "596         -0.966784         -0.959299         -0.948135         -0.977574   \n",
       "253         -0.376459          0.084024         -0.300975         -0.429321   \n",
       "151         -0.359032         -0.124112         -0.102868         -0.394230   \n",
       "994         -0.996173         -0.969167         -0.980864         -0.996487   \n",
       "794         -0.382623          0.070014         -0.478552         -0.392092   \n",
       "733         -0.978131         -0.882129         -0.940289         -0.983558   \n",
       "220         -0.996429         -0.992409         -0.992814         -0.996527   \n",
       "297         -0.011734         -0.261857         -0.196767         -0.097958   \n",
       "884         -0.996723         -0.991117         -0.983270         -0.997237   \n",
       "221         -0.995667         -0.991870         -0.996358         -0.995761   \n",
       "335         -0.112943          0.147021         -0.261996         -0.199658   \n",
       "185         -0.995531         -0.977026         -0.986679         -0.996326   \n",
       "63          -0.995237         -0.995810         -0.994430         -0.995515   \n",
       "243         -0.986413         -0.992749         -0.983236         -0.986959   \n",
       "358         -0.949404         -0.859235         -0.786842         -0.955574   \n",
       "525         -0.953150         -0.899854         -0.959204         -0.961445   \n",
       "597         -0.991619         -0.989278         -0.992551         -0.993212   \n",
       "456         -0.298798         -0.115901         -0.317805         -0.336247   \n",
       "528         -0.992043         -0.935871         -0.970479         -0.994291   \n",
       "330          0.215876          0.092032         -0.296456          0.116780   \n",
       "12          -0.999135         -0.984680         -0.996274         -0.999077   \n",
       "795         -0.334456         -0.078653         -0.550542         -0.338711   \n",
       "105         -0.198107         -0.004638         -0.185038         -0.246645   \n",
       "56          -0.994928         -0.998359         -0.992011         -0.995586   \n",
       "412         -0.980346         -0.935654         -0.966177         -0.982792   \n",
       "..                ...               ...               ...               ...   \n",
       "758         -0.987043         -0.987328         -0.984720         -0.987481   \n",
       "637         -0.191444         -0.155987         -0.358816         -0.275245   \n",
       "115         -0.220770          0.123136         -0.221638         -0.271588   \n",
       "246         -0.956753         -0.939516         -0.763049         -0.956820   \n",
       "64          -0.661294         -0.713353         -0.701155         -0.693948   \n",
       "656         -0.034277         -0.067031         -0.352049         -0.120583   \n",
       "9           -0.994803         -0.972758         -0.986244         -0.995405   \n",
       "155         -0.406022         -0.083614         -0.012498         -0.466763   \n",
       "433         -0.994767         -0.993026         -0.995245         -0.995300   \n",
       "36          -0.996076         -0.995782         -0.998222         -0.996287   \n",
       "305         -0.333353          0.053256         -0.035072         -0.380100   \n",
       "721         -0.990687         -0.976967         -0.981729         -0.991500   \n",
       "32          -0.808638         -0.572746         -0.626415         -0.806213   \n",
       "21          -0.997763         -0.989957         -0.996586         -0.998291   \n",
       "139          0.063689         -0.017155         -0.230358          0.000198   \n",
       "409         -0.991548         -0.964032         -0.980105         -0.992227   \n",
       "276         -0.358399          0.184799         -0.360064         -0.421587   \n",
       "120         -0.368800         -0.083343         -0.263347         -0.423650   \n",
       "278         -0.299196          0.131415         -0.311024         -0.395266   \n",
       "430         -0.992917         -0.988180         -0.989387         -0.993344   \n",
       "574         -0.996137         -0.965252         -0.973905         -0.996368   \n",
       "493         -0.409452         -0.124976         -0.396023         -0.483295   \n",
       "688         -0.815371         -0.718973         -0.585037         -0.857613   \n",
       "618         -0.317952         -0.105712         -0.338104         -0.367994   \n",
       "398         -0.990860         -0.962100         -0.968136         -0.991492   \n",
       "419         -0.977652         -0.979160         -0.988065         -0.978853   \n",
       "670         -0.298204         -0.105104         -0.372639         -0.383011   \n",
       "31          -0.758393         -0.586537         -0.439110         -0.774118   \n",
       "492         -0.414957         -0.137715         -0.447530         -0.495048   \n",
       "70          -0.599392          0.532506          0.004324         -0.664108   \n",
       "\n",
       "     tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  \\\n",
       "662          0.148488         -0.330685          0.496300  ...   \n",
       "766         -0.979689         -0.986139         -0.920796  ...   \n",
       "802         -0.043545         -0.366387          0.320960  ...   \n",
       "959          0.246300         -0.292639          0.587100  ...   \n",
       "551         -0.905690         -0.982957         -0.932092  ...   \n",
       "596         -0.967659         -0.946119         -0.870466  ...   \n",
       "253          0.043470         -0.266537         -0.148724  ...   \n",
       "151         -0.120456         -0.027792         -0.319136  ...   \n",
       "994         -0.969335         -0.978718         -0.939277  ...   \n",
       "794         -0.036307         -0.428664         -0.252421  ...   \n",
       "733         -0.874706         -0.941625         -0.909069  ...   \n",
       "220         -0.991338         -0.994242         -0.941680  ...   \n",
       "297         -0.317471         -0.183052          0.382388  ...   \n",
       "884         -0.991525         -0.984055         -0.940541  ...   \n",
       "221         -0.991677         -0.996794         -0.939448  ...   \n",
       "335          0.056399         -0.259647          0.221382  ...   \n",
       "185         -0.980057         -0.985809         -0.934475  ...   \n",
       "63          -0.995338         -0.993720         -0.939221  ...   \n",
       "243         -0.992296         -0.983508         -0.927935  ...   \n",
       "358         -0.873891         -0.767291         -0.865585  ...   \n",
       "525         -0.900192         -0.959443         -0.862438  ...   \n",
       "597         -0.989961         -0.992815         -0.931332  ...   \n",
       "456         -0.124901         -0.333654          0.022884  ...   \n",
       "528         -0.934291         -0.971623         -0.926832  ...   \n",
       "330         -0.002253         -0.281541          0.537740  ...   \n",
       "12          -0.982937         -0.996410         -0.943906  ...   \n",
       "795         -0.117800         -0.514075         -0.252745  ...   \n",
       "105         -0.062917         -0.139328         -0.019153  ...   \n",
       "56          -0.998077         -0.992591         -0.938974  ...   \n",
       "412         -0.937887         -0.966048         -0.911852  ...   \n",
       "..                ...               ...               ...  ...   \n",
       "758         -0.986007         -0.987248         -0.934684  ...   \n",
       "637         -0.242165         -0.366597          0.245572  ...   \n",
       "115          0.072742         -0.210427         -0.119485  ...   \n",
       "246         -0.931894         -0.738179         -0.933911  ...   \n",
       "64          -0.706558         -0.755422         -0.891284  ...   \n",
       "656         -0.043077         -0.341470          0.463206  ...   \n",
       "9           -0.973663         -0.985642         -0.940028  ...   \n",
       "155         -0.101567          0.094817         -0.232947  ...   \n",
       "433         -0.993950         -0.995172         -0.939363  ...   \n",
       "36          -0.996370         -0.999036         -0.941617  ...   \n",
       "305          0.104575          0.045298         -0.161963  ...   \n",
       "721         -0.977235         -0.981998         -0.938679  ...   \n",
       "32          -0.533811         -0.579394         -0.916064  ...   \n",
       "21          -0.989669         -0.996700         -0.941472  ...   \n",
       "139         -0.073383         -0.182779          0.460332  ...   \n",
       "409         -0.963608         -0.980810         -0.930330  ...   \n",
       "276          0.095438         -0.358339         -0.257466  ...   \n",
       "120         -0.178759         -0.277213         -0.193089  ...   \n",
       "278          0.088386         -0.302162         -0.104191  ...   \n",
       "430         -0.988862         -0.989551         -0.945556  ...   \n",
       "574         -0.964849         -0.974186         -0.940426  ...   \n",
       "493         -0.153687         -0.349319         -0.254782  ...   \n",
       "688         -0.710190         -0.546371         -0.620883  ...   \n",
       "618         -0.174597         -0.355963         -0.064369  ...   \n",
       "398         -0.968516         -0.967290         -0.929766  ...   \n",
       "419         -0.978020         -0.989085         -0.928237  ...   \n",
       "670         -0.145947         -0.360756          0.069007  ...   \n",
       "31          -0.555350         -0.438736         -0.737152  ...   \n",
       "492         -0.139762         -0.429071         -0.068281  ...   \n",
       "70           0.502260          0.048472         -0.717081  ...   \n",
       "\n",
       "     fBodyBodyGyroJerkMag-skewness()  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "662                        -0.597439                        -0.846621   \n",
       "766                        -0.162046                        -0.616500   \n",
       "802                         0.536388                         0.343544   \n",
       "959                        -0.256430                        -0.577911   \n",
       "551                        -0.090760                        -0.424405   \n",
       "596                         0.363769                        -0.015948   \n",
       "253                        -0.122838                        -0.443445   \n",
       "151                        -0.230811                        -0.621552   \n",
       "994                         0.080477                        -0.185231   \n",
       "794                         0.052274                        -0.338604   \n",
       "733                         0.194594                        -0.136211   \n",
       "220                        -0.647139                        -0.883420   \n",
       "297                        -0.025729                        -0.351122   \n",
       "884                        -0.200881                        -0.533472   \n",
       "221                        -0.523191                        -0.804390   \n",
       "335                        -0.439031                        -0.757817   \n",
       "185                         0.263796                        -0.032810   \n",
       "63                         -0.672820                        -0.890797   \n",
       "243                        -0.400481                        -0.766425   \n",
       "358                         0.219153                        -0.188160   \n",
       "525                        -0.137299                        -0.515811   \n",
       "597                        -0.505631                        -0.855800   \n",
       "456                         0.550369                         0.344301   \n",
       "528                         0.057120                        -0.275847   \n",
       "330                        -0.570777                        -0.861041   \n",
       "12                         -0.686389                        -0.878751   \n",
       "795                        -0.161945                        -0.539289   \n",
       "105                        -0.395030                        -0.649193   \n",
       "56                         -0.900519                        -0.988448   \n",
       "412                        -0.402740                        -0.701041   \n",
       "..                               ...                              ...   \n",
       "758                        -0.510066                        -0.871327   \n",
       "637                        -0.378104                        -0.703402   \n",
       "115                        -0.000361                        -0.412768   \n",
       "246                         0.026643                        -0.351440   \n",
       "64                          0.396355                         0.070405   \n",
       "656                        -0.257050                        -0.670590   \n",
       "9                           0.339526                         0.140452   \n",
       "155                        -0.522033                        -0.821995   \n",
       "433                        -0.762665                        -0.879145   \n",
       "36                         -0.537470                        -0.861926   \n",
       "305                        -0.526320                        -0.820887   \n",
       "721                        -0.484983                        -0.842352   \n",
       "32                          0.143358                        -0.097418   \n",
       "21                         -0.848198                        -0.950247   \n",
       "139                        -0.251705                        -0.668827   \n",
       "409                        -0.539127                        -0.844971   \n",
       "276                        -0.088814                        -0.462908   \n",
       "120                        -0.180428                        -0.542152   \n",
       "278                        -0.215522                        -0.501653   \n",
       "430                         0.031841                        -0.275918   \n",
       "574                        -0.108549                        -0.450341   \n",
       "493                        -0.525978                        -0.819203   \n",
       "688                        -0.071829                        -0.536386   \n",
       "618                         0.351741                         0.070033   \n",
       "398                        -0.095466                        -0.580139   \n",
       "419                        -0.595447                        -0.870533   \n",
       "670                         0.374946                         0.065585   \n",
       "31                          0.482485                         0.256095   \n",
       "492                        -0.352189                        -0.665037   \n",
       "70                          0.063364                        -0.330520   \n",
       "\n",
       "     angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "662                     0.049025                             -0.100834   \n",
       "766                     0.107597                              0.015071   \n",
       "802                    -0.627676                              0.702477   \n",
       "959                    -0.642957                              0.821412   \n",
       "551                     0.119320                             -0.056967   \n",
       "596                    -0.033487                              0.414917   \n",
       "253                     0.242602                             -0.050322   \n",
       "151                    -0.183312                             -0.366714   \n",
       "994                     0.010776                             -0.255061   \n",
       "794                     0.311820                             -0.870191   \n",
       "733                    -0.007073                             -0.284287   \n",
       "220                     0.540635                              0.441628   \n",
       "297                     0.533347                              0.848965   \n",
       "884                     0.010811                             -0.110814   \n",
       "221                     0.519163                             -0.770630   \n",
       "335                     0.296343                             -0.860122   \n",
       "185                    -0.035751                             -0.368238   \n",
       "63                      0.170203                              0.310625   \n",
       "243                     0.044117                             -0.096297   \n",
       "358                     0.026818                             -0.421327   \n",
       "525                    -0.064355                              0.030715   \n",
       "597                     0.067526                             -0.104690   \n",
       "456                     0.476859                              0.083179   \n",
       "528                     0.079812                             -0.049976   \n",
       "330                     0.325617                             -0.675525   \n",
       "12                     -0.077552                             -0.101222   \n",
       "795                    -0.598557                             -0.041629   \n",
       "105                    -0.035510                             -0.760585   \n",
       "56                     -0.327661                             -0.689303   \n",
       "412                    -0.027746                              0.132856   \n",
       "..                           ...                                   ...   \n",
       "758                     0.020384                             -0.288375   \n",
       "637                    -0.337421                              0.695591   \n",
       "115                     0.684841                              0.424516   \n",
       "246                     0.008330                             -0.398115   \n",
       "64                      0.033488                              0.061568   \n",
       "656                    -0.771521                             -0.898913   \n",
       "9                      -0.020590                             -0.127730   \n",
       "155                     0.498854                              0.179758   \n",
       "433                     0.098904                              0.768345   \n",
       "36                     -0.008144                              0.559318   \n",
       "305                    -0.362818                             -0.720317   \n",
       "721                     0.190303                              0.223932   \n",
       "32                     -0.004913                              0.033988   \n",
       "21                     -0.002320                              0.150391   \n",
       "139                     0.378196                              0.894064   \n",
       "409                    -0.078489                              0.528527   \n",
       "276                     0.146485                             -0.690094   \n",
       "120                     0.054138                             -0.492607   \n",
       "278                    -0.365812                              0.805359   \n",
       "430                    -0.018798                             -0.055183   \n",
       "574                     0.043198                              0.000724   \n",
       "493                     0.866832                              0.357184   \n",
       "688                     0.051518                              0.535813   \n",
       "618                     0.577947                              0.172898   \n",
       "398                    -0.021361                              0.056178   \n",
       "419                    -0.171237                              0.015460   \n",
       "670                    -0.448352                              0.758061   \n",
       "31                      0.114760                              0.114436   \n",
       "492                    -0.374908                             -0.392331   \n",
       "70                      0.152830                              0.149602   \n",
       "\n",
       "     angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "662                         -0.950340                             -0.073240   \n",
       "766                          0.225829                              0.283668   \n",
       "802                          0.916323                             -0.682592   \n",
       "959                         -0.954953                             -0.885661   \n",
       "551                         -0.580625                              0.582217   \n",
       "596                         -0.271416                              0.545930   \n",
       "253                         -0.226059                             -0.462972   \n",
       "151                         -0.932361                              0.257390   \n",
       "994                          0.253623                              0.376215   \n",
       "794                         -0.814202                             -0.340962   \n",
       "733                         -0.322336                              0.614726   \n",
       "220                         -0.212556                             -0.498513   \n",
       "297                         -0.409820                             -0.130922   \n",
       "884                          0.406559                              0.088131   \n",
       "221                         -0.574786                              0.844139   \n",
       "335                          0.730540                             -0.315334   \n",
       "185                          0.466299                             -0.302831   \n",
       "63                           0.663555                              0.337821   \n",
       "243                          0.035648                             -0.146606   \n",
       "358                         -0.373610                             -0.502968   \n",
       "525                         -0.807828                              0.489175   \n",
       "597                          0.046823                              0.389168   \n",
       "456                         -0.093855                             -0.775732   \n",
       "528                          0.833767                             -0.585575   \n",
       "330                         -0.964238                              0.664562   \n",
       "12                           0.639084                              0.765485   \n",
       "795                          0.624940                             -0.601020   \n",
       "105                          0.535230                             -0.957050   \n",
       "56                           0.397035                             -0.310911   \n",
       "412                         -0.113760                             -0.227883   \n",
       "..                                ...                                   ...   \n",
       "758                          0.263906                             -0.669137   \n",
       "637                          0.700064                             -0.655065   \n",
       "115                         -0.284663                             -0.021918   \n",
       "246                         -0.161538                              0.146414   \n",
       "64                          -0.514438                              0.240943   \n",
       "656                          0.948149                             -0.420604   \n",
       "9                           -0.482871                             -0.070670   \n",
       "155                         -0.818937                             -0.056916   \n",
       "433                          0.281229                              0.399057   \n",
       "36                           0.142527                             -0.016025   \n",
       "305                         -0.949112                             -0.691951   \n",
       "721                         -0.007977                             -0.050694   \n",
       "32                          -0.036068                              0.205113   \n",
       "21                           0.142331                             -0.853711   \n",
       "139                          0.812622                             -0.424187   \n",
       "409                          0.349670                             -0.171902   \n",
       "276                         -0.886370                              0.500656   \n",
       "120                          0.615095                              0.346940   \n",
       "278                          0.323233                              0.009463   \n",
       "430                         -0.597092                              0.556851   \n",
       "574                         -0.069545                             -0.128496   \n",
       "493                          0.909559                             -0.750959   \n",
       "688                          0.044502                              0.019167   \n",
       "618                         -0.089264                             -0.211844   \n",
       "398                         -0.217529                              0.401879   \n",
       "419                         -0.322279                              0.349761   \n",
       "670                          0.941960                             -0.677067   \n",
       "31                          -0.070782                             -0.001461   \n",
       "492                          0.405123                              0.777508   \n",
       "70                           0.127245                              0.192852   \n",
       "\n",
       "     angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  subject  \n",
       "662             -0.746880              0.237799              0.126425        3  \n",
       "766              0.599143             -0.856702             -0.136294        5  \n",
       "802             -0.987978              0.086802              0.022081        5  \n",
       "959             -0.894334              0.166254             -0.010086        5  \n",
       "551             -0.697313              0.298516              0.078604        3  \n",
       "596              0.370914             -0.665645             -0.291669        3  \n",
       "253             -0.704414              0.289528              0.090365        1  \n",
       "151             -0.695481              0.303214              0.065482        1  \n",
       "994             -0.796369              0.213405             -0.065949        6  \n",
       "794             -0.959472              0.117295              0.040097        5  \n",
       "733             -0.859050              0.041942             -0.068427        5  \n",
       "220              0.276305             -0.577603             -0.350193        1  \n",
       "297             -0.742147              0.261729              0.090251        1  \n",
       "884             -0.740473             -0.049976             -0.091703        5  \n",
       "221              0.276640             -0.577824             -0.350103        1  \n",
       "335             -0.705980              0.284190              0.100972        1  \n",
       "185             -0.681004              0.318471              0.022790        1  \n",
       "63               0.397791             -0.543991             -0.433791        1  \n",
       "243              0.436493             -0.814771              0.179584        1  \n",
       "358             -0.744851              0.262502             -0.047606        3  \n",
       "525             -0.750181              0.270219              0.031970        3  \n",
       "597              0.372217             -0.654501             -0.304835        3  \n",
       "456             -0.721141              0.265173              0.116227        3  \n",
       "528             -0.745210              0.273560              0.033208        3  \n",
       "330             -0.684240              0.295148              0.113016        1  \n",
       "12              -0.850654              0.187611             -0.035998        1  \n",
       "795             -0.964063              0.116167              0.036567        5  \n",
       "105             -0.773199              0.254039              0.034529        1  \n",
       "56               0.398897             -0.546859             -0.431054        1  \n",
       "412              0.480300             -0.531092             -0.465743        3  \n",
       "..                    ...                   ...                   ...      ...  \n",
       "758              0.602412             -0.855684             -0.137260        5  \n",
       "637             -0.807519              0.225151              0.058854        3  \n",
       "115             -0.766180              0.258603              0.037376        1  \n",
       "246              0.444490             -0.786322              0.217390        1  \n",
       "64               0.205759             -0.536104             -0.360736        1  \n",
       "656             -0.734896              0.253871              0.116931        3  \n",
       "9               -0.848294              0.190310             -0.034417        1  \n",
       "155             -0.698389              0.304878              0.044325        1  \n",
       "433              0.453743             -0.661493             -0.322760        3  \n",
       "36              -0.314575             -0.218935             -0.323988        1  \n",
       "305             -0.613925              0.350310              0.104406        1  \n",
       "721             -0.363524             -0.289190             -0.182467        5  \n",
       "32              -0.418368             -0.142549             -0.305884        1  \n",
       "21              -0.762023              0.262170              0.029987        1  \n",
       "139             -0.750713              0.267236              0.051523        1  \n",
       "409              0.482282             -0.530567             -0.466610        3  \n",
       "276             -0.687830              0.299898              0.095160        1  \n",
       "120             -0.774100              0.253689              0.031547        1  \n",
       "278             -0.711755              0.285024              0.087985        1  \n",
       "430              0.451020             -0.663451             -0.319994        3  \n",
       "574             -0.935384              0.074459              0.060654        3  \n",
       "493             -0.615279              0.343461              0.120002        3  \n",
       "688             -0.849254              0.114041             -0.090878        5  \n",
       "618             -0.719842              0.256581              0.131992        3  \n",
       "398             -0.866386              0.028249             -0.049782        3  \n",
       "419              0.440432             -0.659452             -0.321601        3  \n",
       "670             -0.618231              0.335930              0.132129        3  \n",
       "31              -0.552939             -0.053539             -0.260424        1  \n",
       "492             -0.615527              0.344476              0.117040        3  \n",
       "70               0.508733             -0.496132             -0.506197        1  \n",
       "\n",
       "[999 rows x 562 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(\"subject\", axis=1)\n",
    "test_data = test_data.drop(\"subject\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8728728728728729"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.score(test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 6)\n",
      "(999, 6)\n"
     ]
    }
   ],
   "source": [
    "n_input = train_data.shape[1] # number of features\n",
    "n_output = 6 # number of possible labels\n",
    "n_samples = train_data.shape[0] # number of training samples\n",
    "n_hidden_units = 40\n",
    "train_label = to_categorical(train_label)\n",
    "test_label = to_categorical(test_label)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden_units,\n",
    "                    input_dim=n_input,\n",
    "                    activation=\"relu\"))\n",
    "    model.add(Dense(n_hidden_units,\n",
    "                    input_dim=n_input,\n",
    "                    activation=\"relu\"))\n",
    "    model.add(Dense(n_output, activation=\"softmax\"))\n",
    "    # Compile model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8718718663827555\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=create_model, epochs=20, batch_size=10, verbose=False)\n",
    "estimator.fit(train_data, train_label)\n",
    "print(\"Score: {}\".format(estimator.score(test_data, test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Feature matrix\n",
    "train_data = train.iloc[:,:561].as_matrix()\n",
    "test_data = test.iloc[:,:561].as_matrix()\n",
    "\n",
    "train_labels = train.iloc[:,562:].as_matrix()\n",
    "test_labels = test.iloc[:,562:].as_matrix()\n",
    "\n",
    "train_labelss=np.zeros((len(train_labels),6))\n",
    "test_labelss=np.zeros((len(test_labels),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kn accuracy_score: 0.8728728728728729\n"
     ]
    }
   ],
   "source": [
    "##### K-Nearest Neighbors ######\n",
    "clf = KNeighborsClassifier(n_neighbors=24)\n",
    "\n",
    "clf.fit(train_data , train_labels)\n",
    "y_te_pred = clf.predict(test_data)\n",
    "\n",
    "knn= accuracy_score(test_labels, y_te_pred)\n",
    "print(\"kn accuracy_score:\",knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9039039039039038\n"
     ]
    }
   ],
   "source": [
    "###### Random Forest #######\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(train_data , train_labels)\n",
    "y_te_pred = rf.predict(test_data)\n",
    "\n",
    "Random = accuracy_score(test_labels, y_te_pred)\n",
    "print(\"Random Forest Accuracy:\",Random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian accuracy score: 0.7597597597597597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\honey\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "###### GaussianNB #######\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNB = GaussianNB()\n",
    "GNB.fit(train_data,train_labels)\n",
    "\n",
    "y_te_pred = GNB.predict(test_data)\n",
    "\n",
    "Gaussian = accuracy_score(test_labels, y_te_pred)\n",
    "print(\"Gaussian accuracy score:\",Gaussian)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7467467467467468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(train_data,train_labels)\n",
    "\n",
    "y_te_pred = DT.predict(test_data)\n",
    "\n",
    "Decision = accuracy_score(test_labels, y_te_pred)\n",
    "print(\"Decision Tree Accuracy:\",Decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "import pickle\n",
    "filename = \"Random Forest\"\n",
    "fileobj = open(filename,'wb')\n",
    "pickle.dump(Random,fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Random Forest\"\n",
    "fileobj = open(filename,'rb')\n",
    "rf = pickle.load(fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9039039039039038\n"
     ]
    }
   ],
   "source": [
    "print(Random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
